#!/usr/bin/env python
# -*- coding: utf-8

"""
Copyright (C) 2014, PaPi Authors

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation; either version 2 of the License, or (at your option)
any later version.

Please read the COPYING file.
"""

import os
import sys
import pysam
import hashlib
import subprocess
import PaPi.contig
import PaPi.fastalib as u
import PaPi.utils as utils
import PaPi.dictio as dictio
import PaPi.filesnpaths as filesnpaths 
import PaPi.terminal as terminal 
from PaPi.constants import IS_ESSENTIAL_FIELD, IS_AUXILIARY_FIELD

from PaPi.profiler import __version__

pp = terminal.pretty_print
kmers = PaPi.kmers.KMers()
P = lambda x, y: os.path.join(x['output_dir'], y)

class MultipleRuns:
    def __init__(self, args):
        self.progress = terminal.Progress()
        self.run = terminal.Run()

        self.project_names = []
        self.project_runinfos = {}
        self.project_runinfo_dict_paths = args.input
        self.runinfo_dict = {}
        self.contigs = {}
        self.profiles = []
        self.output_dir = args.output_dir

    def read_runinfo_dict(self, path):
        runinfo = dictio.read_serialized_object(path)
        project_name = runinfo['project_name']
        if not project_name in self.project_names:
            self.project_names.append(project_name)
        self.project_runinfos[project_name] = runinfo
        return project_name, runinfo


    def read_runinfo_dicts(self):
        improper = []
        missing_path = []
        bad_profiler_version = []

        for p in self.project_runinfo_dict_paths:
            project_name, runinfo = self.read_runinfo_dict(p)
            try:
                project_name, runinfo = self.read_runinfo_dict(p)
            except:
                improper.append(p)
                continue

            # if things are not where they should be, we attempt to reset the directory paths.
            if not os.path.exists(P(runinfo, runinfo['metadata_txt'])):
                new_output_dir = os.path.dirname(os.path.join(os.getcwd(), p))
                old_output_dir = runinfo['output_dir']

                runinfo = filesnpaths.reset_output_dir(p, old_output_dir, new_output_dir)

                # if the paths are still f'd up, store it as bad.
                if not os.path.exists(P(runinfo, runinfo['metadata_txt'])):
                    missing_path.append(p)

            if not runinfo.has_key('profiler_version') or runinfo['profiler_version'] != PaPi.profiler.__version__:
                bad_profiler_version.append(p)

        if improper:
            raise utils.ConfigError, "%s seem to be properly formatted PaPi object: %s. Are you\
                                           sure these are PaPi RUNINFO.cp files?" % \
                                           ('Some RUNINFO files do not' if len(improper) > 1 else "RUNINFO file does not",
                                            ', '.join(improper))

        if missing_path:
            raise utils.ConfigError, "%d of %d files you provided have bad file paths: %s. PaPi tried to fix them, but\
                                           it failed. Maybe these RUNINFO.cp files were generated on another\
                                           machine? Or maybe they were edited manually? Well, PaPi is as lost as you\
                                           are at this point :(" % (len(missing_path),
                                                                    len(self.project_runinfos),
                                                                    ', '.join(['"%s"' % x for x in missing_path]))

        if bad_profiler_version:
            raise utils.ConfigError, "%d of %d RUNINFO.cp files you provided seems to be generated by an\
                                           older version of PaPi profiler that is not compatible with the current\
                                           merger anymore. You need to re-run PaPi profiler on these projects: %s" \
                                                        % (len(bad_profiler_version),
                                                           len(self.project_runinfos),
                                                           ', '.join(['"%s"' % x for x in bad_profiler_version]))


    def sanity_check(self):
        if not self.output_dir:
            raise utils.ConfigError, "Sorry. You must declare an output directory path."
        if not len(self.project_runinfo_dict_paths) > 1:
            raise utils.ConfigError, "You need to provide at least 2 RUNINFO.cp files for this program\
                                           to be useful."

        missing = [p for p in self.project_runinfo_dict_paths if not os.path.exists(p)]
        if missing:
            raise utils.ConfigError, "%s not found: %s." % ('Some files are' if len(missing) > 1 else "File is",
                                                                 ', '.join(missing))

        self.read_runinfo_dicts()

        contigs = {}
        for metadata in [P(self.project_runinfos[r], self.project_runinfos[r]['metadata_txt']) for r in self.project_names]:
            contigs[metadata] = sorted([l.split('\t')[0] for l in open(metadata).readlines()[1:]])

        if len(set([contigs[m].__str__() for m in contigs])) > 1:
            raise utils.ConfigError, "It seems there is at least one run among all runs you declared that differ\
                                           from the others with respect to contig names or number of contigs it contains.\
                                           (I simply checked  all the metadata files whether their first columns match\
                                           each other with respect to their sorted content, so it is not likely that the\
                                           mistake is on my part). If PaPi runs were not performed on BAM files that were\
                                           generated by mapping short reads to the same scaffold, you can't use this\
                                           script. Similarly, PaPi runs with different -M or -C parameters may not be merged."


        self.output_dir = utils.ABS(self.output_dir)
        filesnpaths.gen_output_directory(self.output_dir)


    def merge(self):
        self.sanity_check()

        self.split_names = self.get_split_names()
        self.metadata_fields, self.metadata_for_each_run = self.read_metadata_files()
        self.split_parents = self.get_split_parents()

        self.progress.new('Reading contigs into memory')
        self.read_contigs()
        self.progress.end()

        self.run.info('profiler_version', __version__)
        self.run.info('output_dir', self.output_dir)
        self.run.info('cmd_line', utils.get_cmd_line())
        self.run.info('num_runs_processed', len(self.contigs))
        self.run.info('num_splits_found', pp(len(self.contigs.values()[0])))
        self.run.info('contigs_total_length', pp(sum([len(s) for s in self.contigs.values()[0]])))
 
        self.progress.new('Generating merged summary')
        summary_dir, profile_summary_index = self.merge_split_summaries()
        self.progress.end()
        self.run.info('profile_summary_dir', summary_dir)
        self.run.info('profile_summary_index', profile_summary_index)

        self.progress.new('Analyzing contigs for consensus')
        splits_fasta = self.store_splits_consensus()
        self.progress.end()
        self.run.info('splits_fasta', splits_fasta)

        self.progress.new('GC content for consensus splits')
        self.progress.update('Computing...')
        self.GC_content_for_splits = utils.get_GC_content_for_FASTA_entries(splits_fasta)
        self.progress.end()

        self.progress.new('Analyzing TNF of contigs')
        tnf_matrix = self.store_contigs_tnf()
        self.progress.end()
        self.run.info('tnf_matrix', tnf_matrix)
 
        self.progress.new('Generating TNF tree')
        tnf_tree = self.generate_tnf_tree()
        self.progress.end()
        self.run.info('tnf_tree', tnf_tree)

        self.store_project_runinfos_for_each_metadata_column()
        self.progress.end()
        self.run.quit()


    def store_project_runinfos_for_each_metadata_column(self):
        essential_fields = [f for f in self.metadata_fields if IS_ESSENTIAL_FIELD(f)]
        auxiliary_fields = [f for f in self.metadata_fields if IS_AUXILIARY_FIELD(f)]
        # FIXME: these are pretty embarrassing, and should be fixed at some point:
        Length = "length"
        GC_content = "GC_content"

        for essential_field in essential_fields:
            self.progress.new('Storing RUNINFO dicts for each metadata column')
            self.progress.update('%s ...' % essential_field)
            metadata_output_path = os.path.join(self.output_dir, '-'.join(['METADATA', essential_field]) + '.txt')
            metadata_output = open(metadata_output_path, 'w')
            fields = '\t'.join(['contigs'] + [Length, GC_content] + self.project_names + auxiliary_fields)
            metadata_output.write('%s\n' % fields)
            for split_name in self.split_names:
                line = '\t'.join([split_name] +\
                                 [self.metadata_for_each_run[self.project_names[0]][split_name][Length]] +\
                                 [str(self.GC_content_for_splits[split_name])] +\
                                 [self.metadata_for_each_run[project][split_name][essential_field] for project in self.project_names] +\
                                 [self.metadata_for_each_run[project][split_name][f] for f in auxiliary_fields])
                metadata_output.write('%s\n' % line)
            metadata_output.close()

            self.progress.end()

            runinfo_output_path = os.path.join(self.output_dir, '-'.join(['RUNINFO', essential_field]) + '.cp')
            self.run.info('project_name', '%s (%s)' % (os.path.basename(self.output_dir), essential_field))
            self.run.info('metadata_txt', metadata_output_path)
            self.run.info('runinfo', runinfo_output_path)
            self.run.store_info_dict(runinfo_output_path, strip_prefix = self.output_dir)


    def merge_split_summaries(self):
        merged_summary_index = {}
        merged_summary_index_path = os.path.join(self.output_dir, 'SUMMARY.cp')
        summary_dir = filesnpaths.gen_output_directory(os.path.join(self.output_dir, 'SUMMARY'), delete_if_exits = True)

        
        # read all index files per run into a dict here, so the access is easier from within
        # the for loop below
        run_sum_indices = {}
        for runinfo  in self.project_runinfos.values():
            r = P(runinfo, runinfo['profile_summary_index'])
            run_sum_indices[runinfo['project_name']] = dictio.read_serialized_object(r)

        for i in range(0, len(self.split_names)):
            self.progress.update('merging summaries for splits %s of %s' % (i + 1, len(self.split_names)))
            split_name = self.split_names[i]

            merged_summary = {}
            for runinfo in self.project_runinfos.values():
                r = P(runinfo, run_sum_indices[runinfo['project_name']][split_name])
                run_split_summary = dictio.read_serialized_object(r)
                merged_summary[runinfo['project_name']] = run_split_summary[runinfo['project_name']]

            merged_split_summary_path = os.path.join(summary_dir, os.path.basename(run_sum_indices[runinfo['project_name']][split_name]))
            dictio.write_serialized_object(merged_summary, merged_split_summary_path)
            merged_summary_index[split_name] = merged_split_summary_path

        self.progress.update('Serializing merged split summary index ...')
        dictio.write_serialized_object(dictio.strip_prefix_from_dict_values(merged_summary_index, self.output_dir),\
                                           merged_summary_index_path)

        return summary_dir, merged_summary_index_path


    def generate_tnf_tree(self):
        self.progress.update('...')
        newick_tree_file_path = os.path.join(self.output_dir, 'TNF-NEWICK-TREE.txt')
        utils.get_newick_tree_data(self.run.info_dict['tnf_matrix'], newick_tree_file_path)
        return newick_tree_file_path


    def get_split_parents(self):
        parents = {}
        m = self.metadata_for_each_run.values()[0]
        for split in m:
            parents[split] = m[split]["__parent__"]
        return parents


    def read_metadata_files(self):
        metadata_for_each_run = {}
        m = P(self.project_runinfos.values()[0], self.project_runinfos.values()[0]['metadata_txt'])
        metadata_fields = open(m).readline().strip().split('\t')

        for project_runinfo in self.project_runinfos.values():
            metadata = {}
            metadata_path = P(project_runinfo, project_runinfo['metadata_txt'])
            metadata_obj = open(metadata_path)

            #skip the first line
            metadata_obj.readline()

            for line in metadata_obj.readlines():
                fields = line.strip().split('\t')
                metadata[fields[0]] = {}
                for i in range(1, len(fields)):
                    metadata_field = metadata_fields[i]
                    metadata[fields[0]][metadata_field] = fields[i]
            metadata_for_each_run[project_runinfo['project_name']] = metadata
        return metadata_fields, metadata_for_each_run


    def get_split_names(self):
        m = P(self.project_runinfos[self.project_names[0]], self.project_runinfos[self.project_names[0]]['metadata_txt'])
        return [line.strip().split('\t')[0] for line in open(m).readlines()][1:]


    def read_contigs(self):
        for i in range(0, len(self.project_names)):
            self.progress.update('%d of %d' % (i + 1, len(self.project_names)))
            project_name = self.project_names[i]
            runinfo = self.project_runinfos[project_name]
            fasta = u.SequenceSource(P(runinfo, runinfo['splits_fasta']))
            contigs = {}
            while fasta.next():
                contigs[fasta.id] = fasta.seq
            self.contigs[project_name] = contigs


    def store_contigs_tnf(self):
        contigs_tnf = {}
        num_splits = len(self.split_names)

        # what we call contig, is actually a split. we can't run TNF analysis on a split; instead,
        # we analyze TNF on the parent contig where each split is coming form. it is handled very
        # clearly in papi-profiler, but unfortunately I have been doing some pretty horrible coding
        # here for no reason. so, we alraedy identified which split is associated with what parent
        # contig at this point. now we will generate a parents dict where we will keep properly
        # merged split seqs and their TNF. because we will concatenate split seqs the way they are
        # ordered, it is critical for metadata file to be sorted properly (a later split in order
        # should not appear in the METADATA file earlier). OK. First, lets put sequences in:
        parents_dict = {}
        for i in range(0, num_splits):
            if (i + 1) % 10 == 0 or (i + 1) == num_splits:
                self.progress.update('Generating merged split consensus sequences :: %.2f%%' % ((i + 1) * 100.0/ num_splits))

            split_name = self.split_names[i]
            parent = self.split_parents[split_name]
            
            if parents_dict.has_key(parent):
                parents_dict[parent]['seq'] += self.consensus_splits[split_name]
            else:
                parents_dict[parent] = {'seq': self.consensus_splits[split_name]}

        # ok. now parents_dict contains all the sequences. lets get TNF info:
        parents = parents_dict.keys()
        num_parents = len(parents)
        for i in range(0, num_parents):
            self.progress.update('Computing TNF for merged splits :: %.2f%%' % ((i + 1) * 100.0/ num_parents))

            parent = parents[i]
            parents_dict[parent]['tnf'] = kmers.get_kmer_frequency(parents_dict[parent]['seq'])


        # now we are going to populate the TNF matrix with splits, but using their parent tnf:
        self.progress.update('Storing the matrix ...')
        TNF_matrix_file_path = os.path.join(self.output_dir, 'TETRANUCLEOTIDE-FREQ-MATRIX.txt')
        output = open(TNF_matrix_file_path, 'w')
        keys = kmers.kmers.values()[0]
        output.write('contigs\t%s\n' % ('\t'.join(keys)))
        for split_name in self.split_names:
            parent = self.split_parents[split_name]
            output.write('%s\t' % (split_name))
            output.write('%s\n' % '\t'.join([str(parents_dict[parent]['tnf'][key]) for key in keys]))
        output.close()

        return TNF_matrix_file_path


    def store_splits_consensus(self):
        self.consensus_splits = {}
        num_splits = len(self.split_names)
        for i in range(0, num_splits):
            if (i + 1) % 10 == 0:
                self.progress.update('%.2f%%' % ((i + 1) * 100.0/ num_splits))
            
            split_name = self.split_names[i]

            sequences = set()

            for run in self.contigs:
                sequences.add(self.contigs[run][split_name])
            contig_length = len(self.contigs[run][split_name])

            if len(sequences) == 1:
                # perfect. all consensus sequences recovered by PaPi profiler for each sample is identical
                # this happens only when all reads from all samples were mapped on the contig.
                self.consensus_splits[split_name] = sequences.pop()
            else:
                # this means there are more than one consensus sequences. they are going to be identical,
                # except some of them will have N's because not all samples mapped at all regions of the
                # consensus.
                #
                # (inline FIXME: ^^ this information is critical to retain and visualize.)
                #
                # now these need to be merged into contigs that have no N's.
                #
                new_consensus = ''
                for j in range(0, contig_length):
                    nt_list = [seq[j] for seq in sequences if seq[j] != 'N']
                    if nt_list:
                        new_consensus += nt_list[0]
                    else:
                        new_consensus += 'N'
                self.consensus_splits[split_name] = new_consensus

        self.progress.update('Storing contigs ...')
        output_file_path = os.path.join(self.output_dir, 'CONTIGS-CONSENSUS.fa')
        output = open(output_file_path, 'w')
        for split_name in self.consensus_splits:
            output.write('>%s\n%s\n' % (split_name, self.consensus_splits[split_name]))
        output.close()

        return output_file_path


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Merge multiple PaPi runs to do cross-sectional or timeseries comparisons')
    parser.add_argument('input', metavar = 'RUNINFO_FILE', nargs='+',
                        help = 'PaPi RUNINFO files to create a merged output')
    parser.add_argument('-o', '--output-dir', default = None,
                        help = 'Output directory for merged output')

    args = parser.parse_args()

    try:
        MultipleRuns(args).merge()
    except utils.ConfigError, e:
        print e
        sys.exit(-1)

