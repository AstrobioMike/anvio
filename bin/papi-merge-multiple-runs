#!/usr/bin/env python
# -*- coding: utf-8

"""
Copyright (C) 2014, PaPi Authors

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation; either version 2 of the License, or (at your option)
any later version.

Please read the COPYING file.
"""

import os
import sys
import pysam
import hashlib
import subprocess
import PaPi.utils
import PaPi.contig
import PaPi.fastalib as u
from PaPi.utils import pretty_print as pp 
from PaPi.profiler import __version__

kmers = PaPi.utils.KMers()

class MultipleRuns:
    def __init__(self, args):
        self.progress = PaPi.utils.Progress()
        self.run = PaPi.utils.Run()

        self.project_names = []
        self.project_runinfos = {}
        self.project_runinfo_dict_paths = args.input
        self.runinfo_dict = {}
        self.contigs = {}
        self.profiles = []
        self.output_dir = args.output_dir

    def read_runinfo_dict(self, path):
        runinfo = PaPi.utils.read_serialized_object(path)
        project_name = runinfo['project_name']
        if not project_name in self.project_names:
            self.project_names.append(project_name)
        self.project_runinfos[project_name] = runinfo
        return project_name, runinfo


    def read_runinfo_dicts(self):
        improper = []
        missing_path = []
        bad_profiler_version = []
        for p in self.project_runinfo_dict_paths:
            try:
                project_name, runinfo = self.read_runinfo_dict(p)
            except:
                improper.append(p)

            # if things are not where they should be, we attempt to reset the directory paths.
            if not os.path.exists(runinfo['metadata_txt']):
                PaPi.utils.reset_output_dir(p)

                # now directory paths are fixed in the RUNINFO file, lets read it again.
                project_name, runinfo = self.read_runinfo_dict(p)

                # if the paths are still f'd up, store it as bad.
                if not os.path.exists(runinfo['metadata_txt']):
                    missing_path.append(p)

            if not runinfo.has_key('profiler_version') or runinfo['profiler_version'] != PaPi.profiler.__version__:
                bad_profiler_version.append(p)


        if improper:
            raise PaPi.utils.ConfigError, "%s seem to be properly formatted PaPi object: %s. Are you\
                                           sure these are PaPi RUNINFO.cp files?" % \
                                           ('Some RUNINFO files do not' if len(improper) > 1 else "RUNINFO file does not",
                                            ', '.join(improper))

        if missing_path:
            raise PaPi.utils.ConfigError, "%d of %d files you provided have bad file paths:\n\n%s\n\n. PaPi tried to fix them, but\
                                           it failed. Maybe these RUNINFO.cp files were generated on another\
                                           machine? Or maybe they were edited manually? Well, PaPi is as lost as you\
                                           are at this point :(" % (len(missing_path),
                                                                    len(self.project_runinfos),
                                                                    ', '.join(['"%s"' % x for x in missing_path]))

        if bad_profiler_version:
            raise PaPi.utils.ConfigError, "%d of %d RUNINFO.cp files you provided seems to be generated by an\
                                           older version of PaPi profiler that is not compatible with the current\
                                           merger anymore. You need to re-run PaPi profiler on these projects: %s" \
                                                        % (len(bad_profiler_version),
                                                           len(self.project_runinfos),
                                                           ', '.join(['"%s"' % x for x in bad_profiler_version]))


    def sanity_check(self):
        if not self.output_dir:
            raise PaPi.utils.ConfigError, "Sorry. You must declare an output directory path."
        if not len(self.project_runinfo_dict_paths) > 1:
            raise PaPi.utils.ConfigError, "You need to provide at least 2 RUNINFO.cp files for this program\
                                           to be useful."

        missing = [p for p in self.project_runinfo_dict_paths if not os.path.exists(p)]
        if missing:
            raise PaPi.utils.ConfigError, "%s not found: %s." % ('Some files are' if len(missing) > 1 else "File is",
                                                                 ', '.join(missing))

        self.read_runinfo_dicts()

        contigs = {}
        for metadata in [self.project_runinfos[r]['metadata_txt'] for r in self.project_names]:
            contigs[metadata] = sorted([l.split('\t')[0] for l in open(metadata).readlines()[1:]])

        if len(set([contigs[m].__str__() for m in contigs])) > 1:
            raise PaPi.utils.ConfigError, "It seems there is at least one run among all runs you declared that differ\
                                           from the others with respect to contig names or number of contigs it contains.\
                                           (I simply checked  all the metadata files whether their first columns match\
                                           each other with respect to their sorted content, so it is not likely that the\
                                           mistake is on my part). If PaPi runs were not performed on BAM files that were\
                                           generated by mapping short reads to the same scaffold, you can't use this\
                                           script. Similarly, PaPi runs with differnt -M or -C parameters may not be merged."


        self.output_dir = PaPi.utils.ABS(self.output_dir)
        PaPi.utils.gen_output_directory(self.output_dir)


    def merge(self):
        self.sanity_check()

        self.split_names = self.get_split_names()
        self.metadata_fields, self.metadata_for_each_run = self.read_metadata_files()
        self.split_parents = self.get_split_parents()

        self.progress.new('Reading contigs into memory')
        self.read_contigs()
        self.progress.end()

        self.run.info('profiler_version', __version__)
        self.run.info('output_dir', self.output_dir)
        self.run.info('num_runs_processed', len(self.contigs))
        self.run.info('num_splits_found', pp(len(self.contigs.values()[0])))
        self.run.info('contigs_total_length', pp(sum([len(s) for s in self.contigs.values()[0]])))
 
        self.progress.new('Analyzing contigs for consensus')
        splits_fasta = self.store_contigs_consensus()
        self.progress.end()
        self.run.info('splits_fasta', splits_fasta)
 
        self.progress.new('Analyzing TNF of contigs')
        tnf_matrix = self.store_contigs_tnf()
        self.progress.end()
        self.run.info('tnf_matrix', tnf_matrix)
 
        self.progress.new('Generating TNF tree')
        tnf_tree = self.generate_tnf_tree()
        self.progress.end()
        self.run.info('tnf_tree', tnf_tree)

        self.store_project_runinfos_for_each_metadata_column()
        self.progress.end()
        self.run.quit()


    def store_project_runinfos_for_each_metadata_column(self):
        essential_fields = [f for f in self.metadata_fields if not f.startswith('__') and f != "contigs"]
        auxiliary_fields = [f for f in self.metadata_fields if f.startswith('__')]

        for essential_field in essential_fields:
            self.progress.new('Storing RUNINFO dicts for each metadata column')
            self.progress.update('%s ...' % essential_field)
            metadata_output_path = os.path.join(self.output_dir, '-'.join(['METADATA', essential_field]) + '.txt')
            metadata_output = open(metadata_output_path, 'w')
            fields = '\t'.join(['contigs'] + self.project_names + auxiliary_fields)
            metadata_output.write('%s\n' % fields)
            for split_name in self.split_names:
                line = '\t'.join([split_name] + [self.metadata_for_each_run[project][split_name][essential_field] for project in self.project_names] + [self.metadata_for_each_run[project][split_name][f] for f in auxiliary_fields])
                metadata_output.write('%s\n' % line)
            metadata_output.close()

            self.progress.end()

            runinfo_output_path = os.path.join(self.output_dir, '-'.join(['RUNINFO', essential_field]) + '.cp')
            self.run.info('project_name', essential_field)
            self.run.info('metadata_txt', metadata_output_path)
            self.run.info('runinfo', runinfo_output_path)
            self.run.store_info_dict(runinfo_output_path)


    def generate_tnf_tree(self):
        self.progress.update('...')
        newick_tree_file_path = os.path.join(self.output_dir, 'TNF-NEWICK-TREE.txt')
        PaPi.utils.get_newick_tree_data(self.run.info_dict['tnf_matrix'], newick_tree_file_path)
        return newick_tree_file_path


    def get_split_parents(self):
        parents = {}
        m = self.metadata_for_each_run.values()[0]
        for split in m:
            parents[split] = m[split]["__parent__"]
        return parents


    def read_metadata_files(self):
        metadata_for_each_run = {}
        metadata_fields = open(self.project_runinfos.values()[0]['metadata_txt']).readline().strip().split('\t')

        for project_runinfo in self.project_runinfos.values():
            metadata = {}
            metadata_path = project_runinfo['metadata_txt']
            metadata_obj = open(metadata_path)

            #skip the first line
            metadata_obj.readline()

            for line in metadata_obj.readlines():
                fields = line.strip().split('\t')
                metadata[fields[0]] = {}
                for i in range(1, len(fields)):
                    metadata_field = metadata_fields[i]
                    metadata[fields[0]][metadata_field] = fields[i]
            metadata_for_each_run[project_runinfo['project_name']] = metadata
        return metadata_fields, metadata_for_each_run


    def get_split_names(self):
        return [line.strip().split('\t')[0] for line in open(self.project_runinfos[self.project_names[0]]['metadata_txt']).readlines()][1:]


    def read_contigs(self):
        for i in range(0, len(self.project_names)):
            self.progress.update('%d of %d' % (i + 1, len(self.project_names)))
            project_name = self.project_names[i]
            runinfo = self.project_runinfos[project_name]
            fasta = u.SequenceSource(runinfo['splits_fasta'])
            contigs = {}
            while fasta.next():
                contigs[fasta.id] = fasta.seq
            self.contigs[project_name] = contigs


    def store_contigs_tnf(self):
        contigs_tnf = {}
        num_splits = len(self.split_names)

        # what we call contig, is actually a split. we can't run TNF analysis on a split; instead,
        # we analyze TNF on the parent contig where each split is coming form. it is handled very
        # clearly in papi-profiler, but unfortunately I have been doing some pretty horrible coding
        # here for no reason. so, we alraedy identified which split is associated with what parent
        # contig at this point. now we will generate a parents dict where we will keep properly
        # merged split seqs and their TNF. because we will concatenate split seqs the way they are
        # ordered, it is critical for metadata file to be sorted properly (a later split in order
        # should not appear in the METADATA file earlier). OK. First, lets put sequences in:
        parents_dict = {}
        for i in range(0, num_splits):
            if (i + 1) % 10 == 0 or (i + 1) == num_splits:
                self.progress.update('Generating merged split consensus sequences :: %.2f%%' % ((i + 1) * 100.0/ num_splits))

            split_name = self.split_names[i]
            parent = self.split_parents[split_name]
            
            if parents_dict.has_key(parent):
                parents_dict[parent]['seq'] += self.consensus_splits[split_name]
            else:
                parents_dict[parent] = {'seq': self.consensus_splits[split_name]}

        # ok. now parents_dict contains all the sequences. lets get TNF info:
        parents = parents_dict.keys()
        num_parents = len(parents)
        for i in range(0, num_parents):
            self.progress.update('Computing TNF for merged splits :: %.2f%%' % ((i + 1) * 100.0/ num_parents))

            parent = parents[i]
            parents_dict[parent]['tnf'] = kmers.get_kmer_frequency(parents_dict[parent]['seq'])


        # now we are going to populate the TNF matrix with splits, but using their parent tnf:
        self.progress.update('Storing the matrix ...')
        TNF_matrix_file_path = os.path.join(self.output_dir, 'TETRANUCLEOTIDE-FREQ-MATRIX.txt')
        output = open(TNF_matrix_file_path, 'w')
        keys = kmers.kmers.values()[0]
        output.write('contigs\t%s\n' % ('\t'.join(keys)))
        for split_name in self.split_names:
            parent = self.split_parents[split_name]
            output.write('%s\t' % (split_name))
            output.write('%s\n' % '\t'.join([str(parents_dict[parent]['tnf'][key]) for key in keys]))
        output.close()

        return TNF_matrix_file_path


    def store_contigs_consensus(self):
        self.consensus_splits = {}
        num_splits = len(self.split_names)
        for i in range(0, num_splits):
            if (i + 1) % 10 == 0:
                self.progress.update('%.2f%%' % ((i + 1) * 100.0/ num_splits))
            
            split_name = self.split_names[i]

            sequences = set()

            for run in self.contigs:
                sequences.add(self.contigs[run][split_name])
            contig_length = len(self.contigs[run][split_name])

            if len(sequences) == 1:
                # perfect. all consensus sequences recovered by PaPi profiler for each sample is identical
                # this happens only when all reads from all samples were mapped on the contig.
                self.consensus_splits[split_name] = sequences.pop()
            else:
                # this means there are more than one consensus sequences. they are going to be identical,
                # except some of them will have N's because not all samples mapped at all regions of the
                # consensus.
                #
                # (inline FIXME: ^^ this information is critical to retain and visualize.)
                #
                # now these need to be merged into contigs that have no N's.
                #
                new_consensus = ''
                for j in range(0, contig_length):
                    nt_list = [seq[j] for seq in sequences if seq[j] != 'N']
                    if nt_list:
                        new_consensus += nt_list[0]
                    else:
                        new_consensus += 'N'
                self.consensus_splits[split_name] = new_consensus

        self.progress.update('Storing contigs ...')
        output_file_path = os.path.join(self.output_dir, 'CONTIGS-CONSENSUS.fa')
        output = open(output_file_path, 'w')
        for split_name in self.consensus_splits:
            output.write('>%s\n%s\n' % (split_name, self.consensus_splits[split_name]))
        output.close()

        return output_file_path


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Merge multiple PaPi runs to do cross-sectional or timeseries comparisons')
    parser.add_argument('input', metavar = 'RUNINFO_FILE', nargs='+',
                        help = 'PaPi RUNINFO files to create a merged output')
    parser.add_argument('-o', '--output-dir', default = None,
                        help = 'Output directory for merged output')

    args = parser.parse_args()

    try:
        MultipleRuns(args).merge()
    except PaPi.utils.ConfigError, e:
        print e
        sys.exit(-1)

