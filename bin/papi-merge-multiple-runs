#!/usr/bin/env python
# -*- coding: utf-8

"""
Copyright (C) 2014, PaPi Authors

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation; either version 2 of the License, or (at your option)
any later version.

Please read the COPYING file.
"""

import os
import sys
import pysam
import hashlib
import subprocess
import PaPi.contig
import PaPi.fastalib as u
import PaPi.utils as utils
import PaPi.dictio as dictio
import PaPi.terminal as terminal
import PaPi.constants as constants
import PaPi.clustering as clustering
import PaPi.filesnpaths as filesnpaths
from PaPi.clusteringconfuguration import ClusteringConfiguration

from PaPi.profiler import __version__

pp = terminal.pretty_print
kmers = PaPi.kmers.KMers()
P = lambda x, y: os.path.join(x['output_dir'], y)


class MultipleRuns:
    def __init__(self, args):
        self.progress = terminal.Progress()
        self.run = terminal.Run()

        self.sample_ids = []
        self.sample_runinfos = {}
        self.sample_runinfo_dict_paths = args.input
        self.runinfo_dict = {}
        self.contigs = {}
        self.profiles = []
        self.output_directory = args.output_dir

        self.clustering_configs = constants.clustering_configs['merged']


    def read_runinfo_dict(self, path):
        runinfo = dictio.read_serialized_object(path)
        sample_id = runinfo['sample_id']
        if not sample_id in self.sample_ids:
            self.sample_ids.append(sample_id)
        self.sample_runinfos[sample_id] = runinfo
        return sample_id, runinfo


    def read_runinfo_dicts(self):
        improper = []
        missing_path = []
        bad_profiler_version = []

        for p in self.sample_runinfo_dict_paths:
            sample_id, runinfo = self.read_runinfo_dict(p)
            try:
                sample_id, runinfo = self.read_runinfo_dict(p)
            except:
                improper.append(p)
                continue

            # if things are not where they should be, we attempt to reset the directory paths.
            if not os.path.exists(P(runinfo, runinfo['metadata_splits'])):
                new_output_dir = os.path.dirname(os.path.join(os.getcwd(), p))
                old_output_dir = runinfo['output_dir']

                runinfo = filesnpaths.reset_output_dir(p, old_output_dir, new_output_dir)

                # if the paths are still f'd up, store it as bad.
                if not os.path.exists(P(runinfo, runinfo['metadata_splits'])):
                    missing_path.append(p)

            if not runinfo.has_key('profiler_version') or runinfo['profiler_version'] != PaPi.profiler.__version__:
                bad_profiler_version.append(p)

        if improper:
            raise utils.ConfigError, "%s seem to be properly formatted PaPi object: %s. Are you\
                                           sure these are PaPi RUNINFO.cp files?" % \
                                           ('Some RUNINFO files do not' if len(improper) > 1 else "RUNINFO file does not",
                                            ', '.join(improper))

        if missing_path:
            raise utils.ConfigError, "%d of %d files you provided have bad file paths: %s. PaPi tried to fix them, but\
                                           it failed. Maybe these RUNINFO.cp files were generated on another\
                                           machine? Or maybe they were edited manually? Well, PaPi is as lost as you\
                                           are at this point :(" % (len(missing_path),
                                                                    len(self.sample_runinfos),
                                                                    ', '.join(['"%s"' % x for x in missing_path]))

        if bad_profiler_version:
            raise utils.ConfigError, "%d of %d RUNINFO.cp files you provided seems to be generated by an\
                                           older version of PaPi profiler that is not compatible with the current\
                                           merger anymore. You need to re-run PaPi profiler on these projects: %s" \
                                                        % (len(bad_profiler_version),
                                                           len(self.sample_runinfos),
                                                           ', '.join(['"%s"' % x for x in bad_profiler_version]))


    def sanity_check(self):
        if not self.output_directory:
            raise utils.ConfigError, "Sorry. You must declare an output directory path."
        if not len(self.sample_runinfo_dict_paths) > 1:
            raise utils.ConfigError, "You need to provide at least 2 RUNINFO.cp files for this program\
                                           to be useful."

        missing = [p for p in self.sample_runinfo_dict_paths if not os.path.exists(p)]
        if missing:
            raise utils.ConfigError, "%s not found: %s." % ('Some files are' if len(missing) > 1 else "File is",
                                                                 ', '.join(missing))

        self.read_runinfo_dicts()

        if [True for v in self.sample_runinfos.values() if v['merged']]:
            raise utils.ConfigError, "This is very cute, but you can't merge already merged runs. PaPi can only merge\
                                      individual profiles (which are generated through papi-profile program). Sorry."

        contigs = {}
        for metadata in [P(self.sample_runinfos[r], self.sample_runinfos[r]['metadata_splits']) for r in self.sample_ids]:
            contigs[metadata] = sorted([l.split('\t')[0] for l in open(metadata).readlines()[1:]])

        if len(set([contigs[m].__str__() for m in contigs])) > 1:
            raise utils.ConfigError, "It seems there is at least one run among all runs you declared that differ\
                                      from the others with respect to contig names or number of contigs it contains.\
                                      (I simply checked  all the metadata files whether their first columns match\
                                      each other with respect to their sorted content, so it is not likely that the\
                                      mistake is on my part). If PaPi runs were not performed on BAM files that were\
                                      generated by mapping short reads to the same scaffold, you can't use this\
                                      script. Similarly, PaPi runs with different -M or -C parameters may not be merged."


        self.output_directory = utils.ABS(self.output_directory)
        filesnpaths.gen_output_directory(self.output_directory)


    def merge(self):
        self.sanity_check()

        self.split_names = self.get_split_names()

        # get metadata information for both contigs and splits:
        self.metadata_fields, self.metadata_for_each_run = self.read_metadata_files()
        self.split_parents = self.get_split_parents()

        self.progress.new('Reading contigs into memory')
        self.read_contigs()
        self.progress.end()

        self.run.info('profiler_version', __version__)
        self.run.info('output_dir', self.output_directory)
        self.run.info('merged', True)
        self.run.info('merged_sample_ids', self.sample_ids)
        self.run.info('cmd_line', utils.get_cmd_line())
        self.run.info('num_runs_processed', len(self.contigs))
        self.run.info('num_splits_found', pp(len(self.contigs.values()[0])))
        self.run.info('contigs_total_length', pp(sum([len(s) for s in self.contigs.values()[0]])))
 
        self.progress.new('Generating merged summary')
        summary_dir, profile_summary_index = self.merge_split_summaries()
        self.progress.end()
        self.run.info('profile_summary_dir', summary_dir)
        self.run.info('profile_summary_index', profile_summary_index)

        self.progress.new('Analyzing contigs for consensus')
        splits_fasta = self.store_splits_consensus()
        self.progress.end()
        self.run.info('splits_fasta', splits_fasta)

        self.progress.new('GC content for consensus splits')
        self.progress.update('Computing...')
        self.GC_content_for_splits = utils.get_GC_content_for_FASTA_entries(splits_fasta)
        self.progress.end()

        self.progress.new('Analyzing TNF of contigs')
        tnf_matrices = self.store_tnf_matrices_for_contigs_and_splits()
        self.progress.end()
        for target in tnf_matrices:
             self.run.info('tnf_matrix_%s' % target, tnf_matrices[target])

        self.merged_metadata_files = self.merge_metadata_files()
        self.cluster_contigs()
        self.store_runinfo_dicts()

        self.progress.end()
        self.run.quit()


    def merge_metadata_files(self):
        essential_fields = [f for f in self.metadata_fields if constants.IS_ESSENTIAL_FIELD(f)]
        auxiliary_fields = [f for f in self.metadata_fields if constants.IS_AUXILIARY_FIELD(f)]
        # FIXME: these are pretty embarrassing, and should be fixed at some point:
        Length = "length"
        GC_content = "GC_content"

        merged_metadata_files = {}

        for target in ['contigs', 'splits']:
            self.progress.new('Merging metadata files for %s' % target)
            merged_metadata_files[target] = {}
            for essential_field in essential_fields:
                self.progress.update('%s ...' % essential_field)
                metadata_output_path = os.path.join(self.output_directory, '-'.join(['METADATA', target.upper(), essential_field]) + '.txt')
                merged_metadata_files[target][essential_field] = metadata_output_path
                metadata_output = open(metadata_output_path, 'w')
                fields = '\t'.join(['contigs'] + [Length, GC_content] + self.sample_ids + auxiliary_fields)
                metadata_output.write('%s\n' % fields)
                for split_name in self.split_names:
                    line = '\t'.join([split_name] +\
                                     [self.metadata_for_each_run[target][self.sample_ids[0]][split_name][Length]] +\
                                     [str(self.GC_content_for_splits[split_name])] +\
                                     [self.metadata_for_each_run[target][project][split_name][essential_field] for project in self.sample_ids] +\
                                     [self.metadata_for_each_run[target][project][split_name][f] for f in auxiliary_fields])
                    metadata_output.write('%s\n' % line)
                metadata_output.close()
            self.progress.end()

        return merged_metadata_files


    def cluster_contigs(self):
        # clustering of contigs is done for each configuration file under static/clusterconfigs/merged directory;
        # at this point we don't care what those recipes really require because we already merged and generated
        # every metadata file that may be required.
        clusterings = {}
        for config_name in self.clustering_configs:
            config_path = self.clustering_configs[config_name]
            config = ClusteringConfiguration(config_path, self.output_directory)
            newick_path = clustering.order_contigs_simple(config, progress = self.progress)
            clusterings[config_name] = os.path.basename(newick_path)
        self.run.info('clusterings', clusterings)
        self.run.info('default_clustering', constants.merged_default)


    def store_runinfo_dicts(self):
        # here we store runinfo dicts for essential columns purely for visualizatio purposes.

        essential_fields = [f for f in self.metadata_fields if constants.IS_ESSENTIAL_FIELD(f)]
        for essential_field in essential_fields:
            self.progress.new('Runinfo files are being stored')
            self.progress.update('%s ...' % essential_field)
            runinfo_output_path = os.path.join(self.output_directory, '-'.join(['RUNINFO', essential_field]) + '.cp')
            for target in ['contigs', 'splits']:
                self.run.info('metadata_%s' % target, self.merged_metadata_files[target][essential_field], quiet = True)
            self.progress.end()
            self.run.info('sample_id', '%s (%s)' % (os.path.basename(self.output_directory), essential_field))
            self.run.info('runinfo', runinfo_output_path)
            self.run.store_info_dict(runinfo_output_path, strip_prefix = self.output_directory)


    def merge_split_summaries(self):
        merged_summary_index = {}
        merged_summary_index_path = os.path.join(self.output_directory, 'SUMMARY.cp')
        summary_dir = filesnpaths.gen_output_directory(os.path.join(self.output_directory, 'SUMMARY'), delete_if_exits = True)

        
        # read all index files per run into a dict here, so the access is easier from within
        # the for loop below
        run_sum_indices = {}
        for runinfo  in self.sample_runinfos.values():
            r = P(runinfo, runinfo['profile_summary_index'])
            run_sum_indices[runinfo['sample_id']] = dictio.read_serialized_object(r)

        for i in range(0, len(self.split_names)):
            self.progress.update('merging summaries for splits %s of %s' % (i + 1, len(self.split_names)))
            split_name = self.split_names[i]

            merged_summary = {}
            for runinfo in self.sample_runinfos.values():
                r = P(runinfo, run_sum_indices[runinfo['sample_id']][split_name])
                run_split_summary = dictio.read_serialized_object(r)
                merged_summary[runinfo['sample_id']] = run_split_summary[runinfo['sample_id']]

            merged_split_summary_path = os.path.join(summary_dir, os.path.basename(run_sum_indices[runinfo['sample_id']][split_name]))
            dictio.write_serialized_object(merged_summary, merged_split_summary_path)
            merged_summary_index[split_name] = merged_split_summary_path

        self.progress.update('Serializing merged split summary index ...')
        dictio.write_serialized_object(dictio.strip_prefix_from_dict_values(merged_summary_index, self.output_directory),\
                                           merged_summary_index_path)

        return summary_dir, merged_summary_index_path


    def get_split_parents(self):
        parents = {}
        m = self.metadata_for_each_run['splits'].values()[0]
        for split in m:
            parents[split] = m[split]["__parent__"]
        return parents


    def read_metadata_files(self):
        """reads metadata files of contigs and splits into a dict"""
        metadata_for_each_run = {}

        m = P(self.sample_runinfos.values()[0], self.sample_runinfos.values()[0]['metadata_contigs'])
        metadata_fields = open(m).readline().strip().split('\t')

        for target in ['contigs', 'splits']:
            metadata_for_each_run[target] = {}

            target_metadata = 'metadata_%s' % target

            for sample_runinfo in self.sample_runinfos.values():
                metadata = {}
                metadata_path = P(sample_runinfo, sample_runinfo[target_metadata])
                metadata_obj = open(metadata_path)

                #skip the first line
                metadata_obj.readline()

                for line in metadata_obj.readlines():
                    fields = line.strip().split('\t')
                    metadata[fields[0]] = {}
                    for i in range(1, len(fields)):
                        metadata_field = metadata_fields[i]
                        metadata[fields[0]][metadata_field] = fields[i]
                metadata_for_each_run[target][sample_runinfo['sample_id']] = metadata

        return metadata_fields, metadata_for_each_run


    def get_split_names(self):
        m = P(self.sample_runinfos[self.sample_ids[0]], self.sample_runinfos[self.sample_ids[0]]['metadata_contigs'])
        return [line.strip().split('\t')[0] for line in open(m).readlines()][1:]


    def read_contigs(self):
        for i in range(0, len(self.sample_ids)):
            self.progress.update('%d of %d' % (i + 1, len(self.sample_ids)))
            sample_id = self.sample_ids[i]
            runinfo = self.sample_runinfos[sample_id]
            fasta = u.SequenceSource(P(runinfo, runinfo['splits_fasta']))
            contigs = {}
            while fasta.next():
                contigs[fasta.id] = fasta.seq
            self.contigs[sample_id] = contigs


    def store_tnf_matrices_for_contigs_and_splits(self):
        # what we call a contig, is actually a split. we can't run TNF analysis on a split; instead,
        # we analyze TNF on the parent contig where each split is coming form. it is handled very
        # clearly in papi-profiler, but unfortunately I have been doing some pretty horrible coding
        # here for no reason. so, we already identified which split is associated with what parent
        # contig at this point. now we will generate a parents dict where we will keep properly
        # merged split seqs and their TNF. because we will concatenate split seqs the way they are
        # ordered, it is critical for metadata file to be sorted properly (a later split in order
        # should not appear in the METADATA file earlier). OK. First, lets put sequences in:
        num_splits = len(self.split_names)
        parents_dict = {}
        for i in range(0, num_splits):
            if (i + 1) % 10 == 0 or (i + 1) == num_splits:
                self.progress.update('Generating merged split consensus sequences :: %.2f%%' % ((i + 1) * 100.0/ num_splits))

            split_name = self.split_names[i]
            parent = self.split_parents[split_name]
            
            if parents_dict.has_key(parent):
                parents_dict[parent]['seq'] += self.consensus_splits[split_name]
            else:
                parents_dict[parent] = {'seq': self.consensus_splits[split_name]}

        # ok. now parents_dict contains all the sequences. lets get TNF info:
        parents = parents_dict.keys()
        num_parents = len(parents)
        for i in range(0, num_parents):
            self.progress.update('Computing TNF for merged splits :: %.2f%%' % ((i + 1) * 100.0/ num_parents))

            parent = parents[i]
            parents_dict[parent]['tnf'] = kmers.get_kmer_frequency(parents_dict[parent]['seq'])

        # nice. parents_dict looks like this:
        #
        # {
        #    'contig_1': {'seq': 'ATCATCATC(...)', 'tnf':{'AAAA': 100, 'AAAC': 100, (...)}},
        #    'contig_2': {'seq': 'GATGATGAT(...)', 'tnf':{'AAAA': 100, 'AAAC': 100, (...)}},
        #    (...)
        # }
        #

        # now we are going to populate the TNF matrix with splits, but using their parent tnf:
        output_paths = {}
        for target in ['contigs', 'splits']:
            self.progress.update('Storing tnf matrix for %s ...' % (target))
            matrix_path = os.path.join(self.output_directory, 'TNF-MATRIX-%s.txt' % target.upper())
            output = open(matrix_path, 'w')
            keys = kmers.kmers.values()[0]
            output.write('contigs\t%s\n' % ('\t'.join(keys)))
            for split_name in self.split_names:
                output.write('%s\t' % (split_name))
                if target == "contigs":
                    parent = self.split_parents[split_name]
                    output.write('%s\n' % '\t'.join([str(parents_dict[parent]['tnf'][key]) for key in keys]))
                else:
                    split_tnf = kmers.get_kmer_frequency(self.consensus_splits[split_name])
                    output.write('%s\n' % '\t'.join([str(split_tnf[key]) for key in keys]))
            output.close()
            output_paths[target] = matrix_path

        return output_paths


    def store_splits_consensus(self):
        self.consensus_splits = {}
        num_splits = len(self.split_names)
        for i in range(0, num_splits):
            if (i + 1) % 10 == 0:
                self.progress.update('%.2f%%' % ((i + 1) * 100.0/ num_splits))
            
            split_name = self.split_names[i]

            sequences = set()

            for run in self.contigs:
                sequences.add(self.contigs[run][split_name])
            contig_length = len(self.contigs[run][split_name])

            if len(sequences) == 1:
                # perfect. all consensus sequences recovered by PaPi profiler for each sample is identical
                # this happens only when all reads from all samples were mapped on the contig.
                self.consensus_splits[split_name] = sequences.pop()
            else:
                # this means there are more than one consensus sequences. they are going to be identical,
                # except some of them will have N's because not all samples mapped at all regions of the
                # consensus.
                #
                # (inline FIXME: ^^ this information is critical to retain and visualize.)
                #
                # now these need to be merged into contigs that have no N's.
                #
                new_consensus = ''
                for j in range(0, contig_length):
                    nt_list = [seq[j] for seq in sequences if seq[j] != 'N']
                    if nt_list:
                        new_consensus += nt_list[0]
                    else:
                        new_consensus += 'N'
                self.consensus_splits[split_name] = new_consensus

        self.progress.update('Storing contigs ...')
        output_file_path = os.path.join(self.output_directory, 'CONTIGS-CONSENSUS.fa')
        output = open(output_file_path, 'w')
        for split_name in self.consensus_splits:
            output.write('>%s\n%s\n' % (split_name, self.consensus_splits[split_name]))
        output.close()

        return output_file_path


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Merge multiple PaPi runs to do cross-sectional or timeseries comparisons')
    parser.add_argument('input', metavar = 'RUNINFO_FILE', nargs='+',
                        help = 'PaPi RUNINFO files to create a merged output')
    parser.add_argument('-o', '--output-dir', default = None,
                        help = 'Output directory for merged output')

    args = parser.parse_args()

    try:
        MultipleRuns(args).merge()
    except utils.ConfigError, e:
        print e
        sys.exit(-1)

