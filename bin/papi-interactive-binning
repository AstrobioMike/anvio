#!/usr/bin/env python
# -*- coding: utf-8

"""
Copyright (C) 2014, PaPi Authors

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation; either version 2 of the License, or (at your option)
any later version.

Please read the COPYING file.
"""

import os
import sys
import json
import base64
import shutil
import argparse
import webbrowser
import datetime

from multiprocessing import Process
from bottle import route, static_file, redirect, request, BaseRequest, response
from bottle import run as run_server
from ete2 import Tree

import PaPi.db
import PaPi.profiler
import PaPi.annotation
import PaPi.utils as utils
import PaPi.filesnpaths as filesnpaths
import PaPi.terminal as terminal
import PaPi.dictio as dictio
import PaPi.fastalib as u
from PaPi.constants import levels_of_taxonomy

# get the absolute path for static directory under PaPi
static_dir = os.path.join(os.path.dirname(u.__file__), 'static/interactive')

parser = argparse.ArgumentParser(description='Start PaPi binning interface')
# FIXME: it should be possible to run this without the runinfo-dict.
parser.add_argument('-r', '--runinfo', metavar = 'PATH', default = None,
                    help = 'RUNINFO.cp file generated by a PaPi profiler')
parser.add_argument('-v', '--view', metavar = 'VIEW', default = None,
                    help = 'What view to show on the interface. To see a list of available views, use --show-views flag.')
parser.add_argument('-a', '--annotation-db', metavar = 'DB_FILE', default = None,
                    help = 'Annotation database generated by "papi-gen-annotation". Inclusion of\
                            this file will drastically increase the usefulness of the pipeline.')
parser.add_argument('-f', '--fasta-file', metavar = 'FASTA', default = None,
                    help = 'FASTA file of consensus sequences for each split')
parser.add_argument('-m', '--metadata', metavar = 'TXT', default = None,
                    help = 'TAB-delimited metadata file')
parser.add_argument('-t', '--tree', metavar = 'NEWICK', default = None,
                    help = 'Newick tree of contigs. Declaring a tree file using this parameter will override\
                            the tree file in RUNINFO.')
parser.add_argument('-T', '--taxonomy', metavar = "FILE", default = None,
                    help = "TAB-delimited taxonomy file that contains phylum, class, order, family\
                            genus and species annotations of each contig. Contig names has to match\
                            with the ones in other files, obviously")
parser.add_argument('--title', metavar = "TITLE", default = None,
                    help = "Title for the interface. If you are working with a RUNINFO dict, the title\
                            will be determined based on information stored in that file. Regardless,\
                            you can override that value using this parameter. If you are not using a\
                            PaPi RUNINFO dictionary, a meaningful title will appear in the interface\
                            only if you define one using this parameter.")
parser.add_argument('-A', '--additional-metadata', metavar = "FILE", default = None,
                    help = "A TAB-delimited file for additional metadata for contigs. The file should\
                            contain all contigs that are present in other files.")
parser.add_argument('-S', '--summary-index', metavar = "FILE", default = None,
                     help = "SUMMARY.cp, if there is one available, to inspect contigs from the interface. Will\
                             override the one found in RUNINFO file if it was also declared using -r parameter.")
parser.add_argument('-l', '--taxonomic-level', metavar = 'NAME', default = 'phylum',
                    help = 'The level of taxonomy to annotate the tree. The default is %(default)s.')
parser.add_argument('-o', '--output-dir', metavar = 'DIRECTORY', default = None,
                    help = 'Output directory for output storage')
parser.add_argument('-p', '--port-number', metavar = 'INT', default = 8080, type=int,
                    help = 'Port number to use for communication; the default\
                            (%(default)d) should be OK for almost everyone.')
parser.add_argument('-s', '--state', metavar = "FILE", default = None,
                    help = "State file for loading previous session.")
parser.add_argument('--show-views', action = 'store_true', default = False,
                        help = 'When declared, the program will show a list of available views, and exit.')


progress = terminal.Progress()
run = terminal.Run()

#######################################################################################################################
# Input handler
#######################################################################################################################

class RuninfoIO:
    def __init__(self, args):
        self.runinfo = {}
        self.title = 'Unknown Project'
        self.annotation = None
        self.contig_names_ordered = None
        self.contig_rep_seqs = {}
        self.contigs_summary_index = {}
        self.contig_lengths = {}
        self.taxonomy = None
        self.taxonomic_level = None
        self.additional_metadata_path = None

        self.P = lambda x: os.path.join(self.runinfo['output_dir'], x)

        self.cwd = os.getcwd()

        self.state = args.state

        if args.runinfo:
            # runinfo dict provided: we don't want anything else
            if args.fasta_file or args.metadata:
                raise utils.ConfigError, "You declared a RUNINFO dict with '-r'. You are not allowed to\
                                               declare any of '-f', '-m', or '-t' parameters if you have a\
                                               RUNINFO dict. Please refer to the documentation."

            if not os.path.exists(args.runinfo):
                raise utils.ConfigError, "'%s'? No such file." % (args.runinfo)

            self.runinfo = dictio.read_serialized_object(args.runinfo)

            # if the user wants to see available views, show them and exit.
            if args.show_views:
                num_views = len(self.runinfo['views'])
                print "* %d view%s available for this run is listed below." % (num_views,
                                                                               's' if num_views > 1 else '')
                run.info('Available views', None, header = True)
                for view in self.runinfo['views']:
                    run.info(view, 'Via "%s" table' % self.runinfo['views'][view])
                print
                sys.exit()

            # if the user specifies a view, set it as default:
            if args.view:
                if not args.view in self.runinfo['views']:
                    raise utils.ConfigError, "The requested view ('%s') is not available for this run. Please see\
                                              available views by running this program with --show-views flag." % args.view

                self.runinfo['default_view'] = args.view


            base_dir = os.path.dirname(args.runinfo)
            self.runinfo['output_dir'] = os.path.join(os.getcwd(), base_dir)

            if not self.runinfo.has_key('runinfo'):
                raise utils.ConfigError, "'%s' does not seem to be a PaPi RUNINFO.cp." % (args.runinfo)

            # connect to the PROFILE.db
            self.profile_db = PaPi.db.DB(self.P(self.runinfo['profile_db']), PaPi.profiler.__version__)


            # if there is a tree supplied through the command line, let that tree override what is in the
            # runinfo. kinda sketchy, but it is necessary.
            if args.tree:
                run.info('Warning', "The default tree in RUNINFO is being overriden by '%s'." % args.tree)
                self.runinfo['tree_file'] = os.path.abspath(args.tree)
            else:
                self.runinfo['tree_file'] = self.runinfo['clusterings'][self.runinfo['default_clustering']]

            if args.summary_index:
                run.info('Warning', "The default summary index in RUNINFO is being overriden by '%s'." % args.summary_index)
                self.runinfo['profile_summary_index'] = os.path.abspath(args.summary_index)

            if not self.runinfo.has_key('profiler_version') or self.runinfo['profiler_version'] != PaPi.profiler.__version__:
                raise utils.ConfigError, "RUNINFO.cp seems to be generated from an older version of PaPi\
                                               profiler that is not compatible with the current interactive interface\
                                               anymore. You need to re-run PaPi profiler on these projects."

            if args.title:
                self.title = args.title + ' (%s)' % self.runinfo['default_view']
            else:
                self.title = self.runinfo['sample_id'] + ' (%s)' % self.runinfo['default_view']

            # FIXME: how it came to this point is a long story. but here we are trying to comply with the design
            # that was in place by replacing 'table names' for each self.runinfo['views'] item with TAB delimited
            # file paths on this ... by .. literally .. rading them from the database, and converting them into
            # TAB delimited files, JUST TO BE READ later on as JSON formatted objects. This is ridiculous, because
            # what is being done here is 2 steps with a lot of IO when it could be one very fast step. This
            # has to be fixed at some point (tl;dr, for each view, self.runinfo['views'][view] must contain a JSON
            # formatted output of the 'view' table):
            for view in self.runinfo['views']:
                table = self.runinfo['views'][view]
                table_rows = self.profile_db.get_all_rows_from_table(table)
                tmp_file_path = filesnpaths.get_temp_file_path()
                table_structure = self.profile_db.get_table_structure(table)
                utils.store_array_as_TAB_delimited_file(table_rows, tmp_file_path, table_structure)
                self.runinfo['views'][view] = tmp_file_path

            self.contigs_summary_index = dictio.read_serialized_object(self.P(self.runinfo['profile_summary_index']))

            self.runinfo['self_path'] = args.runinfo

            ###########################################################################################################
            # we're done with call with runinfo
            ###########################################################################################################

        else:
            # self.runinfo *must* contain 'tree_file', 'metadata_json', 'output_dir' and 'self_path'.
            if (not args.fasta_file) or (not args.metadata) or (not args.tree) or (not args.output_dir):
                raise utils.ConfigError, "If you do not have a RUNINFO dict, you must declare each of\
                                               '-f', '-m', '-t' and '-o' parameters. Please see '--help' for\
                                               more detailed information on them."

            if args.view:
                raise utils.ConfigError, "You can't use '-v' parameter when this program is not called with a RUNINFO.cp"

            if args.show_views:
                raise utils.ConfigError, "Sorry, there are no views to show when there is no RUNINFO.cp :/"

            self.runinfo['splits_fasta'] = os.path.abspath(args.fasta_file)
            self.runinfo['output_dir'] = os.path.abspath(args.output_dir)
            self.runinfo['default_view'] = 'single'
            self.runinfo['views'] = {'single': os.path.abspath(args.metadata)}
            self.runinfo['default_clustering'] = 'default'
            self.runinfo['clusterings'] = {'default': os.path.abspath(args.tree)}
            self.runinfo['tree_file'] = os.path.abspath(args.tree)

            if args.summary_index:
                self.runinfo['profile_summary_index'] = os.path.abspath(args.summary_index)
                self.contigs_summary_index = dictio.read_serialized_object(self.runinfo['profile_summary_index'])

            # sanity of the metadata
            #filesnpaths.is_file_json_formatted(args.metadata)
            filesnpaths.is_file_tab_delimited(self.runinfo['views']['single'])
            if not open(self.runinfo['views']['single']).readline().split('\t')[0] == "contig":
                raise utils.ConfigError, "The first row of the first column of the metadata file must\
                                          say 'contig', which is not the case for your metadata file\
                                          ('%s'). Please make sure this is a properly formatted metadata\
                                          file." % (self.runinfo['views']['single'])
            filesnpaths.is_file_fasta_formatted(self.runinfo['splits_fasta'])

            # reminder: this is being stored in the output dir provided as a commandline parameter:
            self.runinfo['self_path'] = os.path.join(self.runinfo['output_dir'], 'RUNINFO.cp')

            if args.title:
                self.title = args.title

            filesnpaths.gen_output_directory(self.runinfo['output_dir'])

            ###########################################################################################################
            # we're done with call with files
            ###########################################################################################################


        # if we have an annotation_db, lets load it up. the split length used when the annotation db
        # created must match with the split length used to profile these merged runs. the problem is,
        # if interactive binning is being called without a runinfo, we are going to have to ask the
        # the user to run interactive interface without an annotation db.
        if args.annotation_db:
            self.annotation = PaPi.annotation.Annotation(args.annotation_db)
            self.annotation.init_database()

            profiling_split_length = int(self.runinfo['split_length'])
            annotation_split_length = int(self.annotation.db.get_meta_value('split_length'))
            if profiling_split_length != annotation_split_length:
                raise utils.ConfigError, "The split length (-L) used to profile these merged runs (which is '%s') seem\
                                          to differ from the split length used to generate %s (which is\
                                          '%s'). Probably the best option is to re-create the annotation database with\
                                          an identical split length parameter." % (terminal.pretty_print(profiling_split_length),
                                                                                   os.path.basename(args.annotation_db),
                                                                                   terminal.pretty_print(annotation_split_length))

            annotation_source = self.annotation.db.get_meta_value('annotation_source')
            run.info('annotation_db initialized', '%s (v. %s) (via "%s")' % (args.annotation_db,
                                                                             self.annotation.db.version,
                                                                             annotation_source))


        # take care of taxonomy if a file is provided. otherwise self.taxonomy remain None
        if args.taxonomy:
            if not args.taxonomic_level:
                raise utils.ConfigError, "When a taxonomy file is declared, a taxonomic level to visualize\
                                               has to be defined."

            self.taxonomic_level = args.taxonomic_level.lower()

            if self.taxonomic_level not in levels_of_taxonomy:
                raise utils.ConfigError, "The taxonomic level '%s' is not a valid one. Taxonomic level has\
                                               to match one of these: %s" % (args.taxonomic_level,
                                                                             levels_of_taxonomy)

            filesnpaths.is_file_tab_delimited(args.taxonomy)
            self.taxonomy = utils.get_TAB_delimited_file_as_dictionary(args.taxonomy, expected_fields = levels_of_taxonomy)

        if args.additional_metadata:
            filesnpaths.is_file_tab_delimited(args.additional_metadata)
            self.additional_metadata_path = args.additional_metadata


        tree = Tree(self.P(self.runinfo['tree_file']))
        self.contig_names_ordered = [n.name for n in tree.get_leaves()]

        self.check_names_consistency()
        self.convert_metadata_into_json()

        fasta = u.SequenceSource(self.P(self.runinfo['splits_fasta']))

        while fasta.next():
            self.contig_rep_seqs[fasta.id] = fasta.seq
            self.contig_lengths[fasta.id] = len(fasta.seq)


    def check_names_consistency(self):
        contigs_in_tree = sorted(self.contig_names_ordered)
        contigs_in_metadata = sorted([l.split('\t')[0] for l in open(self.runinfo['views'][self.runinfo['default_view']]).readlines()[1:]])
        contigs_in_fasta = sorted(utils.get_all_ids_from_fasta(self.P(self.runinfo['splits_fasta'])))

        try:
            assert(contigs_in_fasta == contigs_in_tree == contigs_in_metadata)
        except:
            S = lambda x, y: "agrees" if x == y else "does not agree"
            raise utils.ConfigError, "Contigs name found in the FASTA file, the tree file and the\
                                           metadata needs to match perfectly. It seems it is not the\
                                           case for the input you provided (the metadata %s with the tree,\
                                           the tree %s with the fasta, the fasta %s with the metadata;\
                                           HTH!)." % (S(contigs_in_metadata, contigs_in_tree),
                                                      S(contigs_in_fasta, contigs_in_tree),
                                                      S(contigs_in_metadata, contigs_in_fasta))

        if self.taxonomy:
            contigs_in_taxonomy = set(self.taxonomy.keys())
            for contig in contigs_in_metadata:
                if not contig in contigs_in_taxonomy:
                    raise utils.ConfigError, "Contig names in taxonomy do not include all contig names present\
                                                   in other files. Bad news :/"

        if self.additional_metadata_path:
            contigs_in_additional_metadata = set(sorted([l.split('\t')[0] for l in open(self.additional_metadata_path).readlines()[1:]]))
            for contig in contigs_in_tree:
                if contig not in contigs_in_additional_metadata:
                    raise utils.ConfigError, "Contig names in additional metadata file do not include all contig names present\
                                                   in other files. Bad news :/"


    def update_runinfo_on_disk(self):
        path = self.runinfo.pop('self_path')
        dictio.write_serialized_object(self.runinfo, path)


    def convert_metadata_into_json(self):
        metadata_file_path = self.runinfo['views'][self.runinfo['default_view']]

        if self.taxonomy:
            metadata_headers = utils.get_columns_of_TAB_delim_file(metadata_file_path)
            metadata_dict = utils.get_TAB_delimited_file_as_dictionary(metadata_file_path)
            for contig in metadata_dict.keys():
                metadata_dict[contig][self.taxonomic_level] = self.taxonomy[contig][self.taxonomic_level]

            new_metatada_file_path = filesnpaths.get_temp_file_path()
            headers = ['contigs', self.taxonomic_level] + metadata_headers
            utils.store_dict_as_TAB_delimited_file(metadata_dict, new_metatada_file_path, headers)
            metadata_file_path = new_metatada_file_path


        if self.annotation:
            splits_additional_info = {}
            progress.new('Generating splits additional info')
            num_contigs = len(self.contig_names_ordered)

            metadata_headers = utils.get_columns_of_TAB_delim_file(metadata_file_path)
            metadata_dict = utils.get_TAB_delimited_file_as_dictionary(metadata_file_path)

            splits = self.annotation.db.get_table_as_dict('splits', PaPi.annotation.splits_table_structure)
            for split in metadata_dict:
                for key in PaPi.annotation.splits_table_structure[1:]:
                    metadata_dict[split][key] = splits[split][key]

            new_metatada_file_path = filesnpaths.get_temp_file_path()
            headers = ['contigs'] + PaPi.annotation.splits_table_structure[1:] + metadata_headers
            utils.store_dict_as_TAB_delimited_file(metadata_dict, new_metatada_file_path, headers)
            metadata_file_path = new_metatada_file_path


        if self.additional_metadata_path:
            metadata_headers = utils.get_columns_of_TAB_delim_file(metadata_file_path)
            additional_headers = utils.get_columns_of_TAB_delim_file(self.additional_metadata_path)
            metadata_dict = utils.get_TAB_delimited_file_as_dictionary(metadata_file_path)
            metadata_dict = utils.get_TAB_delimited_file_as_dictionary(self.additional_metadata_path, dict_to_append = metadata_dict)
            
            new_metatada_file_path = filesnpaths.get_temp_file_path()
            headers = ['contigs'] + metadata_headers + additional_headers
            utils.store_dict_as_TAB_delimited_file(metadata_dict, new_metatada_file_path, headers)
            metadata_file_path = new_metatada_file_path


        json_obj = utils.get_json_obj_from_TAB_delim_metadata(metadata_file_path)

        temp_file_path = filesnpaths.get_temp_file_path()
        f = open(temp_file_path, 'w')
        f.write(json_obj)
        f.close()
        self.runinfo['metadata_json'] = temp_file_path


    def end(self):
        # FIXME: remove temp files and stuff
        pass

try:
    d = RuninfoIO(parser.parse_args())
except utils.ConfigError, e:
    print e
    sys.exit(-1)


#######################################################################################################################
# bottle callbacks
#######################################################################################################################

@route('/')
def redirect_to_app():
    redirect('/app/index.html')

@route('/app/:filename#.*#')
def send_static(filename):
    response.set_header('Content-Type', 'application/json')
    response.set_header('Pragma', 'no-cache')
    response.set_header('Cache-Control', 'no-cache, no-store, max-age=0, must-revalidate')
    response.set_header('Expires', 'Thu, 01 Dec 1994 16:00:00 GMT')
    return static_file(filename, root=static_dir)

@route('/data/<name>')
def send_data(name):
    response.set_header('Content-Type', 'application/json')
    response.set_header('Pragma', 'no-cache')
    response.set_header('Cache-Control', 'no-cache, no-store, max-age=0, must-revalidate')
    response.set_header('Expires', 'Thu, 01 Dec 1994 16:00:00 GMT')
    if name == "clusterings":
        return json.dumps((d.runinfo['default_clustering'], d.runinfo['clusterings']), )
    elif name == "meta":
        return static_file(d.runinfo['metadata_json'], root='/')
    elif name == "state":
        if d.state:
            return static_file(os.path.abspath(d.state), root='/')
        return "{}"
    elif name == "contig_lengths":
        return json.dumps(d.contig_lengths)
    elif name == "title":
        return json.dumps(d.title)

@route('/tree/<tree_id>')
def send_data(tree_id):
    response.set_header('Content-Type', 'application/json')
    response.set_header('Pragma', 'no-cache')
    response.set_header('Cache-Control', 'no-cache, no-store, max-age=0, must-revalidate')
    response.set_header('Expires', 'Thu, 01 Dec 1994 16:00:00 GMT')
    if d.runinfo['clusterings'].has_key(tree_id):
        print "* '%s' tree is requested" % (tree_id)
        return static_file(d.P(d.runinfo['clusterings'][tree_id]), root ='/')

@route('/data/charts/<contig_name>')
def charts(contig_name):
    data = {'layers': [],
             'index': None,
             'total': None,
             'coverage': [],
             'variability': [],
             'competing_nucleotides': [],
             'previous_contig_name': None,
             'next_contig_name': None}

    if not d.contigs_summary_index.has_key(contig_name):
        return data

    index_of_contig = d.contig_names_ordered.index(contig_name)
    if index_of_contig:
        data['previous_contig_name'] = d.contig_names_ordered[index_of_contig - 1]
    if (index_of_contig + 1) < len(d.contig_names_ordered):
        data['next_contig_name'] = d.contig_names_ordered[index_of_contig + 1]

    data['index'] = index_of_contig + 1
    data['total'] = len(d.contig_names_ordered)

    contigs = dictio.read_serialized_object(d.P(d.contigs_summary_index[contig_name]))
    layers = sorted(contigs.keys())

    for layer in layers:
        data['layers'].append(layer)
        data['coverage'].append(contigs[layer]['coverage'])
        data['variability'].append(contigs[layer]['variability'])
        data['competing_nucleotides'].append(contigs[layer]['competing_nucleotides'])

    return json.dumps(data)

@route('/data/contig/<contig_name>')
def contig_info(contig_name):
    return d.contig_rep_seqs[contig_name]

@route('/save_state', method='POST')
def save_state():
    state = request.forms.get('state')

    now = datetime.datetime.now()
    state_output_path = os.path.join(d.runinfo['output_dir'], now.strftime("state-%Y-%m-%d-%H-%M-%S.json"))

    state_output = open(state_output_path, 'w')
    state_output.write(state)
    state_output.close()

@route('/submit', method='POST')
def get_data():
    bins = json.loads(request.forms.get('groups'))
    svg = request.forms.get('svg')

    if svg:
        # take care of the SVG.
        svg_output_path = os.path.join(d.runinfo['output_dir'], 'BINS.svg')
        print '* Storing SVG data into "%s"' % svg_output_path
        svg_output = open(svg_output_path, 'w')
        svg_output.write(svg)
        svg_output.close()
        d.runinfo['bins_svg'] = svg_output_path

    if not bins:
        return

    # take care of bins.. start by removing the bins dir if it exists.
    bins_dir = os.path.join(d.runinfo['output_dir'], 'BINS')
    if os.path.exists(bins_dir):
        shutil.rmtree(bins_dir)
    os.makedirs(bins_dir)
    d.runinfo['bins'] = {} 
    print '* Storing bins under "%s"' % bins_dir
    for bin in bins:
        output_path = os.path.join(d.runinfo['output_dir'], 'BINS', bin + '.fa')
        output = open(output_path, 'w')
        for contig in bins[bin]:
            output.write('>%s\n%s\n' % (contig, d.contig_rep_seqs[contig]))
        output.close()
        d.runinfo['bins'][bin] = output_path

    # update RUNINFO dict...
    print '* Storing updated RUNINFO.cp'
    d.update_runinfo_on_disk()
    print


port = parser.parse_args().port_number

port = utils.get_available_port_num(start = port)

if not port:
    print 'Error: PaPi failed to find a port number that is available :('
    sys.exit(-1)
else:
    print '(using port number %d)' % port

# increase maximum size of form data to 100 MB
BaseRequest.MEMFILE_MAX = 1024 * 1024 * 100 

try:
    server_process = Process(target=run_server, kwargs={'host': '127.0.0.1', 'port': port, 'quiet': True})
    server_process.start()
    webbrowser.open_new("http://127.0.0.1:%d" % port)
    print "\n\n -- When you are finished, press CTRL+C to terminate the server..\n\n"
    server_process.join()
except KeyboardInterrupt:
    print("\n\n -- Server is being terminated, please wait.\n\n")
    server_process.terminate()
    sys.exit(1)
