# -*- coding: utf-8
import os
import json
import os.path
import argparse

import numpy as np
import pandas as pd

import anvio
import anvio.utils as u
import anvio.workflows as w

from Bio import SeqIO
from anvio.errors import ConfigError
from anvio.workflows.ecophylo import EcoPhyloWorkflow

__author__ = "Matthew S. Schechter"
__copyright__ = "Copyright 2017, The anvio Project"
__credits__ = []
__license__ = "GPL 3.0"
__version__ = anvio.__version__
__maintainer__ = "Matthew S. Schechter"
__email__ = "mschechter@uchicago.edu"


M = EcoPhyloWorkflow(argparse.Namespace(config=config))
M.init()

dirs_dict = M.dirs_dict

rule ECO_PHYLO_WORKFLOW_target_rule:
    input: M.target_files

rule anvi_run_hmms_hmmsearch:
    """Run hmmsearch with input HMMs to get domtblout"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_run_hmms_hmmsearch-{sample_name}-{HMM}.log")
    input:
    output:
        done = touch(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-{HMM}-dom_hmmsearch/contigs-hmmsearch.done")),
        domtable = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-{HMM}-dom_hmmsearch/hmm.domtable"),
        hmm_table = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-{HMM}-dom_hmmsearch/hmm.table")
    params:
        hmm_source = M.get_param_value_from_config(['anvi_run_hmms_hmmsearch', '--installed-hmm-profile']),
    threads: M.T('anvi_run_hmms_hmmsearch')
    run:
        contigsDB = os.path.join(M.input_dirs_dict[wildcards.sample_name], f"{wildcards.sample_name}-contigs.db")
        HMM_dir = os.path.join(M.HMM_path_dict[wildcards.HMM])
        hmmer_output_dir = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}-{wildcards.HMM}-dom_hmmsearch")

        internal_HMM_sources = ["Bacteria_71", "Archaeal_XX"]

        HMM_source = M.HMM_source_dict[wildcards.HMM]

        # Run different HMM search depending on whether a HMM is internal or external because anvio
        if HMM_source in internal_HMM_sources:
            print(f"Running internal HMM dataset: {HMM_source}")
            HMM_source = M.HMM_source_dict[wildcards.HMM]
            shell(f"anvi-run-hmms -c {contigsDB} \
                                --hmmer-program hmmsearch \
                                --hmmer-output-dir {hmmer_output_dir} \
                                -I {HMM_source} \
                                --domain-hits-table \
                                --just-do-it \
                                -T {threads} 2> {log}")
        else:
            HMM_dir = os.path.join(M.HMM_path_dict[wildcards.HMM])
            print("Running external HMM dataset: ", wildcards.HMM)
            shell(f"anvi-run-hmms -c {contigsDB} \
                            --hmmer-program hmmsearch \
                            --hmm-profile-dir {HMM_dir} \
                            --hmmer-output-dir {hmmer_output_dir} \
                            --domain-hits-table \
                            --just-do-it \
                            -T {threads} 2> {log}")

rule filter_hmm_hits_by_query_coverage:
    """
    Filter hmm_hits table in the contigsDB by query coverage using domtblout.
    This will remove any sketchy sequences that were recruited by the HMM.
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "filter_hmm_hits_by_query_coverage-{sample_name}-{HMM}.log")
    input:
        done = rules.anvi_run_hmms_hmmsearch.output.done,
        domtblout = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-{HMM}-dom_hmmsearch/hmm.domtable")
    output:
        done = touch(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-{HMM}-contigsDB_filtered.done"))
    params:
        query_coverage = M.get_param_value_from_config(['filter_hmm_hits_by_query_coverage', '--query-coverage']),
        additional_params = M.get_param_value_from_config(['filter_hmm_hits_by_query_coverage', 'additional_params'])
    threads: M.T('filter_hmm_hits_by_query_coverage')
    run:
        contigsDB = os.path.join(M.input_dirs_dict[wildcards.sample_name], f"{wildcards.sample_name}-contigs.db")
        domtblout = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}-{wildcards.HMM}-dom_hmmsearch/hmm.domtable")
        HMM_source = M.HMM_source_dict[wildcards.HMM]
        HMM_dir = os.path.join(M.HMM_path_dict[wildcards.HMM])

        # FIXME: need to make a list of all internal sources
        internal_HMM_sources = ["Bacteria_71", "Archaeal_XX"]

        HMM_source = M.HMM_source_dict[wildcards.HMM]

        if HMM_source in internal_HMM_sources:
            shell(f"anvi-script-filter-hmm-hits-table -c {contigsDB} \
                                                    --domain-hits-table {domtblout} \
                                                    --hmm-source {HMM_source} \
                                                    --query-coverage {params.query_coverage} \
                                                    {params.additional_params} 2> {log}")
        else:
            shell(f"anvi-script-filter-hmm-hits-table -c {contigsDB} \
                                                    --domain-hits-table {domtblout} \
                                                    --hmm-profile-dir {HMM_dir} \
                                                    --hmm-source {HMM_source} \
                                                    --query-coverage {params.query_coverage} \
                                                    {params.additional_params} 2> {log}")

rule anvi_get_sequences_for_hmm_hits:
    """Extract all AA and NT sequences that were recruited by the HMM"""

    version: 1.0
    log: 
        AA = os.path.join(dirs_dict['LOGS_DIR'], "anvi_get_sequences_for_hmm_hits_AA-{sample_name}-{HMM}.log"),
        NT = os.path.join(dirs_dict['LOGS_DIR'], "anvi_get_sequences_for_hmm_hits_NT-{sample_name}-{HMM}.log")
    input:
        done_file = rules.filter_hmm_hits_by_query_coverage.output.done,
        contigsDB = ancient(lambda wildcards: os.path.join(M.input_dirs_dict[wildcards.sample_name], "%s-contigs.db" % wildcards.sample_name))
    output:
        done = touch(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits.faa.done")),
        faa = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits.faa"),
        fna = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits.fna"),
    threads: M.T('anvi_get_sequences_for_hmm_hits')
    run:
        internal_HMM_sources = ["Bacteria_71", "Archaeal_XX"]
        HMM_source = M.HMM_source_dict[wildcards.HMM]
        if HMM_source in internal_HMM_sources:
            shell(f"anvi-get-sequences-for-hmm-hits -c {input.contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                --gene-names {wildcards.HMM} \
                                                --get-aa-sequences \
                                                -o {output.faa} 2> {log.AA}")

            shell(f"anvi-get-sequences-for-hmm-hits -c {input.contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                --gene-names {wildcards.HMM} \
                                                -o {output.fna} 2> {log.NT}")
        else:
            shell(f"anvi-get-sequences-for-hmm-hits -c {input.contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                --get-aa-sequences \
                                                -o {output.faa} 2> {log.AA}")

            shell(f"anvi-get-sequences-for-hmm-hits -c {input.contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                -o {output.fna} 2> {log.NT}")


rule simplify_names_from_hmm_hits:
    """Clean up fasta headers for tree calculation"""

    version: 1.0
    log: 
        NT = os.path.join(dirs_dict['LOGS_DIR'], "simplify_names_from_hmm_hits_NT_{sample_name}_{HMM}.log"),
        AA = os.path.join(dirs_dict['LOGS_DIR'], "simplify_names_from_hmm_hits_AA_{sample_name}_{HMM}.log"),
    input: 
        NT = rules.anvi_get_sequences_for_hmm_hits.output.fna,
        AA = rules.anvi_get_sequences_for_hmm_hits.output.faa
    output:
        fasta_NT = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits_renamed.fna"),
        fasta_AA = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits_renamed.faa"),
        report_file_NT = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-reformat_report_nt.txt"),
        report_file_AA = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-reformat_report_AA.txt")
    params:

    threads: M.T('simplify_names_from_scg_hits_nt')
    run:
        prefix_list = [wildcards.sample_name, wildcards.HMM]
        prefix = "_".join(prefix_list) 

        shell(f"anvi-script-reformat-fasta {input.NT} \
                              --simplify-names \
                              --prefix {prefix} \
                              --report-file {output.report_file_NT} \
                              -o {output.fasta_NT} >> {log.NT} 2>&1")

        shell(f"anvi-script-reformat-fasta {input.AA} \
                              --simplify-names \
                              --prefix {prefix} \
                              --report-file {output.report_file_AA} \
                              -o {output.fasta_AA} >> {log.AA} 2>&1")


rule cat_sequences_to_one_fasta:
    """
    Cat all sequences from seperate metagenomes, genomes, SAGs, or MAGs into one fasta
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "cat_sequences_to_one_fasta_{HMM}.log")
    input:
        NT = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{sample_names}-{{HMM}}-hmm_hits_renamed.fna"), sample_names = M.names_list),
        AA = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{sample_names}-{{HMM}}-hmm_hits_renamed.faa"), sample_names = M.names_list),
    output:
        NT_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-all.fna"),
        AA_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-all.faa"),
    threads: M.T('cat_sequences_to_one_fasta')
    run:
        shell(f"cat {input.NT} >> {output.NT_all}")

        shell(f"cat {input.AA} >> {output.AA_all}")


rule anvi_get_external_gene_calls_file:
    """Extract external_gene_calls table from all samples"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "get_external_gene_calls_file_{sample_name}_{HMM}.log")
    input:
        done = rules.anvi_get_sequences_for_hmm_hits.output.done,
    output:
        external_gene_calls = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-external_gene_calls.tsv")
    params:
        fasta = temp(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-orfs.fna")),
    threads: M.T('anvi_get_external_gene_calls_file')
    run:
        contigsDB = os.path.join(M.input_dirs_dict[wildcards.sample_name], f"{wildcards.sample_name}-contigs.db")
        shell('anvi-get-sequences-for-gene-calls -c {contigsDB} \
                                                 --external-gene-calls {output.external_gene_calls} \
                                                 -o {params.fasta} 2> {log}')


rule rename_and_filter_external_gene_calls_file:
    """
    Create primary key from external-gene-calls.txt to join with references sequences to make
    a contigsDB for the metagenomics workflow
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "rename_and_filter_external_gene_calls_file_{sample_name}_{HMM}.log")
    input:
        external_gene_calls = rules.anvi_get_external_gene_calls_file.output.external_gene_calls
    output:
        external_gene_calls_renamed = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-external_gene_calls_renamed.tsv")
    threads: M.T('rename_and_filter_external_gene_calls_file')
    script:
        "scripts/rename_external_gene_calls_file.py"
 

rule cat_external_gene_calls_file:
    """Cat all external_gene_calls files from all samples into one file"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "cat_external_gene_calls_file_{HMM}.log")
    input:
        external_gene_calls_renamed = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{sample_names}-{{HMM}}-external_gene_calls_renamed.tsv"), sample_names = M.names_list),
    output:
        external_gene_calls_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-external_gene_calls_all.tsv"),
    threads: M.T('cat_external_gene_calls_file')
    run:
        shell(f"cat {input.external_gene_calls_renamed} >> {output.external_gene_calls_all} 2> {log}")


rule cluster_X_percent_sim_mmseqs:
    """
    Cluster SCG NT sequences with mmseqs to remove redundant SCG sequences which will prevent non-specific read recruitment
    and read recruitment dilution.
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "cluster_90_mmseqs_{HMM}.log")
    input: rules.cat_sequences_to_one_fasta.output.NT_all
    output: os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-mmseqs_NR_rep_seq.fasta")
    params:
        output_prefix = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-mmseqs_NR"),
        mmseqs_tmp = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-tmp"),
        min_seq_id = M.get_param_value_from_config(['cluster_X_percent_sim_mmseqs', '--min-seq-id'])
    threads: M.T('cluster_X_percent_sim_mmseqs')
    run:
        shell(f"mmseqs easy-cluster {input} \
                                    {params.output_prefix} \
                                    {params.mmseqs_tmp} \
                                    --threads {threads} \
                                    --min-seq-id {params.min_seq_id} >> {log} 2>&1")


rule subset_AA_seqs_with_mmseqs_reps:
    """Subset AA sequences for the mmseqs cluster representatives"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "subset_AA_seqs_with_mmseqs_reps_{HMM}.log")
    input:
        fa = rules.cat_sequences_to_one_fasta.output.AA_all,
        reps = rules.cluster_X_percent_sim_mmseqs.output
    output:
        fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-AA_subset.fa"),
        headers = temp(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-headers.tmp"))
    threads: M.T('subset_AA_seqs_with_mmseqs_reps')
    run:
        shell("grep '>' {input.reps} | sed 's/>//g' > {output.headers}")

        shell("anvi-script-reformat-fasta {input.fa} -I {output.headers} -o {output.fasta} >> {log} 2>&1")


rule align_sequences:
    """MSA of AA sequences subset."""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "align_sequences_{HMM}.log")
	input: rules.subset_AA_seqs_with_mmseqs_reps.output.fasta
	output: os.path.join(dirs_dict['MSA'], "{HMM}-aligned.fa")
	threads: M.T('align_sequences')
	shell:
		"muscle -in {input} -out {output} -maxiters 3 -verbose 2> {log}"


rule trim_alignment:
    """Trim MSA alignment"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "trim_alignment_{HMM}.log")
    input: rules.align_sequences.output
    output: os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_aligned_trimmed.fa")
    params:
        gt = M.get_param_value_from_config(['trim_alignment', '-gt']),
        gappyout = M.get_rule_param('trim_alignment', '-gappyout'),
        additional_params = M.get_param_value_from_config(['trim_alignment', 'additional_params'])
    threads: M.T('trim_alignment')
    shell:
        'trimal -in {input} \
                -out {output} \
                {params.gappyout} \
                {params.additional_params} 2> {log}'


rule remove_sequences_with_X_percent_gaps:
    """Removing sequences that have Z > X% gaps"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "remove_sequences_with_X_percent_gaps_{HMM}.log")
    input: rules.trim_alignment.output
    output: os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_aligned_trimmed_filtered.fa")
    params:
        seq_counts_tsv = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_gaps_counts"),
        max_percentage_gaps = M.get_param_value_from_config(['remove_sequences_with_X_percent_gaps', '--max-percentage-gaps'])
    threads: M.T('remove_sequences_with_X_percent_gaps')
	shell:
		"anvi-script-reformat-fasta {input} \
									-o {output} \
									--max-percentage-gaps {params.max_percentage_gaps} \
									--export-gap-counts-table {params.seq_counts_tsv} >> {log} 2>&1"


rule count_num_sequences_filtered:
    """Record the number of sequences filtered at each step of the workflow"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "count_num_sequences_filtered_{HMM}.log")
    input:
        step1 = rules.cat_sequences_to_one_fasta.output.NT_all,
        step2 = rules.cluster_X_percent_sim_mmseqs.output,
        step3 = rules.remove_sequences_with_X_percent_gaps.output
    output: os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_MSA_STATS'], "{HMM}", "{HMM}_stats.tsv")
    threads: M.T('count_num_sequences_filtered')
    shell:
        """
        # How many SCGs (e.g. Ribosomal_L16) did we recruit across all input data (genomes and metagenomes)?
        step1=$(grep -c '>' {input.step1})
        echo -e "num_input_seqs\t$step1\t{input.step1}\n" > {output}

        # How many SCGs do we have after we cluster them at the nt level and pick representatives?
        step2=$(grep -c '>' {input.step2})
        echo -e "cluster_X_percent_sim_mmseqs\t$step2\t{input.step2}\n" >> {output}

        # How many seqs are in the NR set?
        step3=$(grep -c '>' {input.step3})
        echo -e "remove_sequences_with_X_percent_gaps\t$step3\t{input.step3}\n" >> {output}

        # Add column names here
        echo -e "Rule_name\tNum_sequences\trel_path\n$(cat {output})" > {output}
        """


rule subset_DNA_reps_with_QCd_AA_reps_for_mapping:
    """Filter for SCG NT sequences what will be used for read recruitment later"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "subset_DNA_reps_with_QCd_AA_reps_for_mapping_{HMM}.log")
    input:
        fasta = rules.cat_sequences_to_one_fasta.output.NT_all,
        reps = rules.remove_sequences_with_X_percent_gaps.output
    output:
        NT_for_mapping = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-references_for_mapping_NT.fa"),
        headers = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_headers.tmp")
    threads: M.T('subset_DNA_reps_with_QCd_AA_reps_for_mapping')
    run:
        shell("grep '>' {input.reps} | sed 's/>//g' > {output.headers}")

        shell("anvi-script-reformat-fasta {input.fasta} -I {output.headers} -o {output.NT_for_mapping} >> {log} 2>&1")


rule subset_external_gene_calls_file_all:
    """
    Subset the concatenated exteral_gene_calls.txt for the final set of NT sequences for profiling.
    ALSO, once all external-gene-calls are concatenated, gene-callers-ids will be re-indexed so that 
    there is NO repeating values. 
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "subset_external_gene_calls_file_all_{HMM}.log")
    input:
        external_gene_calls_all = rules.cat_external_gene_calls_file.output.external_gene_calls_all,
        headers = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_headers.tmp")
    output:
        external_gene_calls_subset = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-external_gene_calls_subset.tsv"),
    threads: M.T('subset_external_gene_calls_file_all')
    script:
        "scripts/subset_external_gene_calls_file.py"


rule make_fasta_txt:
    """
    Format a fasta.txt with the filtered NT sequences that will be profiled 
    using the metagenomics workflow.
    """

    version: 1.0
    input:
        expand(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-external_gene_calls_subset.tsv"), HMM = M.HMM_source_dict.keys()),
        expand(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-references_for_mapping_NT.fa"), HMM = M.HMM_source_dict.keys()),
    output:
        fasta_txt = os.path.join("EXTERNAL_ECO_PHYLO_WORKFLOW/METAGENOMICS_WORKFLOW", "fasta.txt"),
    threads: M.T('make_fasta_txt')
    run:
        fastas = [os.path.join("..", "02_NR_FASTAS", r + "/" + r + '-references_for_mapping_NT.fa') for r in M.HMM_source_dict.keys()]
        external_gene_calls = [os.path.join("..", "02_NR_FASTAS", r + "/" + r + '-external_gene_calls_subset.tsv') for r in M.HMM_source_dict.keys()]

        list_of_strings = ['\t'.join(t) + '\n' for t in zip(M.HMM_source_dict.keys(), fastas, external_gene_calls)]

        shell('echo -e "name\tpath\texternal_gene_calls" > {output.fasta_txt}')
        shell('echo -n "%s" >> {output.fasta_txt}' % ''.join(list_of_strings))

if M.run_iqtree == True:
  rule iqtree:
      """Calculate a phylogenetic tree using iqtree"""

      version: 1.0
      log: os.path.join(dirs_dict['LOGS_DIR'], "iqtree_{HMM}.log")
      input: rules.remove_sequences_with_X_percent_gaps.output
      output: 
          tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.iqtree"),
          done = touch(os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}-tree.done"))
      params:
          outfile=os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}"),
          model = M.get_param_value_from_config(['iqtree', '-m']),
          additional_params = M.get_param_value_from_config(['iqtree', 'additional_params'])
      threads: M.T('iqtree')
      shell:
          "iqtree -s {input} \
                  -nt AUTO \
                  -m {params.model} \
                  -pre {params.outfile} \
                  -T AUTO \
                  {params.additional_params} >> {log} 2>&1"

elif M.run_fasttree == True:
  rule fasttree:
      """Want to go faster?? Then Fasttree"""

      version: 1.0
      log: os.path.join(dirs_dict['LOGS_DIR'], "fasttree_{HMM}.log")
      input: rules.remove_sequences_with_X_percent_gaps.output
      output:
          tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.nwk"),
          done = touch(os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}-tree.done"))
      params:
          additional_params = M.get_param_value_from_config(['fasttree', 'additional_params'])
      threads: M.T('fasttree')
      shell:
          "FastTree -fastest {input} 2> {log} 1> {output.tree}"


rule make_metagenomics_config_file:
    """Make a METAGENOMICS WORKFLOW config.json customized for ECO_PHYLO_WORKFLOW"""

    version: 1.0
    input:
        rules.make_fasta_txt.output.fasta_txt
    output:
        config = os.path.join("EXTERNAL_ECO_PHYLO_WORKFLOW/METAGENOMICS_WORKFLOW", "metagenomics_config.json")
    threads: M.T('make_metagenomics_config_file')
    run:

        shell('anvi-run-workflow -w metagenomics --get-default-config {output.config}')

        config = open(output.config)
        config_dict = json.load(config)
        config_dict['fasta_txt'] = 'fasta.txt'
        sample_txt_path = "../../" + M.samples_txt_file
        config_dict['samples_txt'] = sample_txt_path
        config_dict['references_mode'] = True
        config_dict['anvi_run_hmms']['run'] = False
        config_dict["anvi_script_reformat_fasta"]['run'] = False
        config_dict['anvi_run_kegg_kofams']['run'] = False
        config_dict['anvi_run_ncbi_cogs']['run'] = False
        config_dict['anvi_run_scg_taxonomy']['run'] = False
        config_dict['anvi_run_trna_scan']['run'] = False
        config_dict['anvi_run_scg_taxonomy']['run'] = False
        config_dict['iu_filter_quality_minoche']['run'] = False
        config_dict['anvi_profile']['--min-contig-length'] = 0
        config_dict['bowtie']['threads'] = 5

        if M.clusterize_metagenomics_workflow == True:
            config_dict['bowtie']['threads'] = 10
            config_dict['anvi_profile']['threads'] = 10
            config_dict['anvi_merge']['threads'] = 10
        else:
            pass

        with open(output.config, "w") as outfile:
          json.dump(config_dict, outfile, indent=4)


rule run_metagenomics_workflow:
    """Run metagenomics workflow"""

    version: 1.0
    input:
        config = rules.make_metagenomics_config_file.output.config,
    output:
        touch(os.path.join("EXTERNAL_ECO_PHYLO_WORKFLOW/METAGENOMICS_WORKFLOW", "metagenomics_workflow.done"))
    params:
        HPC_string = M.metagenomics_workflow_HPC_string
    threads: M.T('run_metagenomics_workflow')
    run:
        if M.clusterize_metagenomics_workflow == True:
            shell('cd EXTERNAL_ECO_PHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/ && anvi-run-workflow -w metagenomics -c metagenomics_config.json --additional-params --cluster \'clusterize -j={{rule}} -o={{log}} -n={{threads}} -x\' --cores 200 --resource nodes=200 --latency-wait 100 --keep-going --rerun-incomplete && cd -')
        elif M.metagenomics_workflow_HPC_string:
            HPC_string = params.HPC_string
            shell(f'cd EXTERNAL_ECO_PHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/ && anvi-run-workflow -w metagenomics -c metagenomics_config.json --additional-params --cluster "{HPC_string}" --cores 201 --resource nodes=200 --latency-wait 100 --keep-going --rerun-incomplete && cd -')
        else:
            shell('cd EXTERNAL_ECO_PHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/ && anvi-run-workflow -w metagenomics -c metagenomics_config.json -A --rerun-incomplete --latency-wait 100 --keep-going && cd -')