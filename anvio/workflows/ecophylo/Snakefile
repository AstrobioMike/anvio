# -*- coding: utf-8
import os
import json
import os.path
import argparse

import numpy as np
import pandas as pd

import anvio
import anvio.utils as u
import anvio.workflows as w

from Bio import SeqIO
from anvio.dbops import ContigsDatabase
from anvio.errors import ConfigError
from anvio.workflows.ecophylo import EcoPhyloWorkflow

__author__ = "Matthew S. Schechter"
__copyright__ = "Copyright 2017, The anvio Project"
__credits__ = []
__license__ = "GPL 3.0"
__version__ = anvio.__version__
__maintainer__ = "Matthew S. Schechter"
__email__ = "mschechter@uchicago.edu"


M = EcoPhyloWorkflow(argparse.Namespace(config=config))
M.init()

dirs_dict = M.dirs_dict

rule ECOPHYLO_WORKFLOW_target_rule:
    input: M.target_files


rule anvi_run_hmms_hmmsearch:
    """
    Run hmmsearch with input HMMs to get domtblout.
    If incoming contigsDBs do not have `scg_taxonomy_was_run` then run anvi-run-scg-taxonomy 
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_run_hmms_hmmsearch-{sample_name}-{HMM}.log")
    input:
    output:
        done = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-{HMM}-dom_hmmsearch", "{sample_name}-{HMM}-contigs-hmmsearch.done"),
    params:
        hmm_source = M.get_param_value_from_config(['anvi_run_hmms_hmmsearch', '--installed-hmm-profile']),
        additional_params = M.get_param_value_from_config(['trim_alignment', 'additional_params'])
    run:
        contigsDB = os.path.join(M.contigsDB_name_path_dict[wildcards.sample_name])
        HMM_dir = os.path.join(M.HMM_path_dict[wildcards.HMM])
        HMM_source = M.HMM_source_dict[wildcards.HMM]
        hmmer_output_dir = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}-{wildcards.HMM}-dom_hmmsearch")
        domtblout = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}-{wildcards.HMM}-dom_hmmsearch/hmm.domtable")

        if wildcards.sample_name in M.metagenomes_name_list:
            threads = M.get_param_value_from_config(['anvi_run_hmms_hmmsearch', 'threads_metagenomes']),
        else:
            threads = M.get_param_value_from_config(['anvi_run_hmms_hmmsearch', 'threads_genomes']),

        # Run different HMM search depending on whether a HMM is internal or external because anvio
        if HMM_source in M.internal_HMM_sources:
            if not os.path.exists(domtblout):
                print(f"Running internal HMM dataset: {HMM_source}")
                shell("anvi-run-hmms -c {contigsDB} \
                                    --hmmer-program hmmsearch \
                                    --hmmer-output-dir {hmmer_output_dir} \
                                    -I {HMM_source} \
                                    --domain-hits-table \
                                    --just-do-it \
                                    -T {threads} >> {log} 2>&1")
            else:
                pass

            # Load contigsDB
            contigs_db = ContigsDatabase(contigsDB)
            # Check if anvi-run-scg-taxonomy has been run
            scg_taxonomy_was_run_value = contigs_db.meta['scg_taxonomy_was_run']

            if scg_taxonomy_was_run_value != 1:
                print(f"Running anvi-run-scg-taxonomy on {contigsDB} since the {wildcards.HMM} is in anvio's collection of SCGs")
                shell("anvi-run-scg-taxonomy -c {contigsDB} --num-threads {threads} {params.additional_params}")
        else:
            if not os.path.exists(domtblout):
                print(f"Running external HMM dataset: {wildcards.HMM}")
                shell("anvi-run-hmms -c {contigsDB} \
                                    --hmmer-program hmmsearch \
                                    --hmm-profile-dir {HMM_dir} \
                                    --hmmer-output-dir {hmmer_output_dir} \
                                    --domain-hits-table \
                                    --just-do-it \
                                    -T {threads} >> {log} 2>&1")
            else:
                pass
        
        shell("touch {output.done}")


rule filter_hmm_hits_by_query_coverage:
    """
    Filter hmm_hits table in the contigsDB by query coverage using domtblout.
    This will remove any sketchy sequences that were recruited by the HMM.
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "filter_hmm_hits_by_query_coverage-{sample_name}-{HMM}.log")
    input:
        done = rules.anvi_run_hmms_hmmsearch.output.done,
    output:
        done = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-{HMM}-dom_hmmsearch", "{sample_name}-{HMM}-filter_hmm_hits.done")
    params:
        query_coverage = M.get_param_value_from_config(['filter_hmm_hits_by_query_coverage', '--query-coverage']),
        additional_params = M.get_param_value_from_config(['filter_hmm_hits_by_query_coverage', 'additional_params'])
    threads: M.T('filter_hmm_hits_by_query_coverage')
    run:
        contigsDB = os.path.join(M.contigsDB_name_path_dict[wildcards.sample_name])
        HMM_dir = os.path.join(M.HMM_path_dict[wildcards.HMM])
        HMM_source = M.HMM_source_dict[wildcards.HMM]
        domtblout = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}-{wildcards.HMM}-dom_hmmsearch/hmm.domtable")

        if HMM_source in M.internal_HMM_sources:
            shell(f"anvi-script-filter-hmm-hits-table -c {contigsDB} \
                                                    --domain-hits-table {domtblout} \
                                                    --hmm-source {HMM_source} \
                                                    --query-coverage {params.query_coverage} \
                                                    {params.additional_params} >> {log} 2>&1")
        else:
            shell(f"anvi-script-filter-hmm-hits-table -c {contigsDB} \
                                                    --domain-hits-table {domtblout} \
                                                    --hmm-profile-dir {HMM_dir} \
                                                    --hmm-source {HMM_source} \
                                                    --query-coverage {params.query_coverage} \
                                                    {params.additional_params} >> {log} 2>&1")
        shell('touch {output.done}')


rule anvi_get_sequences_for_hmm_hits:
    """
    Extract all AA and NT sequences that were recruited by the HMM

    Snakemake needs to pass through this rule regardless if it's able to extract a 
    target protein from it e.g. I still want the workflow to cat all of the target proteins
    even if a few of the MAGs DO NOT have the target protein. I think one solution would be 
    to touch a done file immediately so that even if the rule fails the file will be there and the
    workflow can move on. This is sketchy though because if the rule fails for another reason
    then we wont know what happened unless the user checks the individual logs
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_get_sequences_for_hmm_hits-{sample_name}-{HMM}.log"),
    input:
        done_file = rules.filter_hmm_hits_by_query_coverage.output.done,
    output:
        done = touch(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits.done")),
        faa = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits.faa"),
        fna = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits.fna"),
    threads: M.T('anvi_get_sequences_for_hmm_hits')
    run:
        HMM_source = M.HMM_source_dict[wildcards.HMM]
        contigsDB = os.path.join(M.contigsDB_name_path_dict[wildcards.sample_name])

        if HMM_source in M.internal_HMM_sources:
            shell("anvi-get-sequences-for-hmm-hits -c {contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                --gene-names {wildcards.HMM} \
                                                --get-aa-sequences \
                                                -o {output.faa} --just-do-it >> {log} 2>&1")

            shell("anvi-get-sequences-for-hmm-hits -c {contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                --gene-names {wildcards.HMM} \
                                                -o {output.fna} --just-do-it >> {log} 2>&1")
        else:
            shell("anvi-get-sequences-for-hmm-hits -c {contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                --get-aa-sequences \
                                                -o {output.faa} --just-do-it >> {log} 2>&1")

            shell("anvi-get-sequences-for-hmm-hits -c {contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                -o {output.fna} --just-do-it >> {log} 2>&1")


rule simplify_names_from_hmm_hits:
    """Clean up fasta headers for tree calculation"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "simplify_names_from_hmm_hits-{sample_name}_{HMM}.log"),
    input: 
        NT = rules.anvi_get_sequences_for_hmm_hits.output.fna,
        AA = rules.anvi_get_sequences_for_hmm_hits.output.faa
    output:
        fasta_NT = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits_renamed.fna"),
        fasta_AA = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits_renamed.faa"),
        report_file_NT = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-reformat_report_nt.txt"),
        report_file_AA = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-reformat_report_AA.txt")
    params:

    threads: M.T('simplify_names_from_hmm_hits')
    run:
        prefix_list = [wildcards.sample_name, wildcards.HMM]
        prefix = "_".join(prefix_list) 

        shell(f"anvi-script-reformat-fasta {input.NT} \
                              --simplify-names \
                              --prefix {prefix} \
                              --report-file {output.report_file_NT} \
                              -o {output.fasta_NT} >> {log} 2>&1")

        shell(f"anvi-script-reformat-fasta {input.AA} \
                              --simplify-names \
                              --prefix {prefix} \
                              --report-file {output.report_file_AA} \
                              -o {output.fasta_AA} >> {log} 2>&1")


rule combine_sequence_data:
    """
    Cat all sequences and reformat_files from seperate metagenomes, genomes, SAGs, or MAGs into one fasta
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "combine_sequence_data_{HMM}.log")
    input:
        NT = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{sample_names}-{{HMM}}-hmm_hits_renamed.fna"), sample_names = M.names_list),
        AA = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{sample_names}-{{HMM}}-hmm_hits_renamed.faa"), sample_names = M.names_list),
        reformat_report_list = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{sample_names}-{{HMM}}-reformat_report_AA.txt"), sample_names = M.names_list),
    output:
        NT_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-all.fna"),
        AA_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-all.faa"),
        reformat_report_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-reformat-report-all.txt")
    threads: M.T('combine_sequence_data')
    run:
        with open(output.NT_all, "a") as NT_output:
            for f in input.NT:
                NT_output.write(open(f).read())
        with open(output.AA_all, "a") as AA_all:
            for f in input.AA:
                AA_all.write(open(f).read())
        with open(output.reformat_report_all, "a") as reformat_report_all:
            for f in input.reformat_report_list:
                reformat_report_all.write(open(f).read())

rule anvi_get_external_gene_calls_file:
    """Extract external_gene_calls table from all samples"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "get_external_gene_calls_file_{sample_name}_{HMM}.log")
    input:
        done = rules.anvi_get_sequences_for_hmm_hits.output.done,
    output:
        external_gene_calls = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-external_gene_calls.tsv")
    params:
        fasta = temp(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-orfs.fna")),
    threads: M.T('anvi_get_external_gene_calls_file')
    run:
        contigsDB = os.path.join(M.contigsDB_name_path_dict[wildcards.sample_name])
        shell('anvi-get-sequences-for-gene-calls -c {contigsDB} \
                                                 --external-gene-calls {output.external_gene_calls} \
                                                 -o {params.fasta} >> {log} 2>&1')


rule rename_and_filter_external_gene_calls_file:
    """
    Create primary key from external-gene-calls.txt to join with references sequences to make
    a contigsDB for the metagenomics workflow
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "rename_and_filter_external_gene_calls_file_{sample_name}_{HMM}.log")
    input:
        reformat_file = rules.simplify_names_from_hmm_hits.output.report_file_AA,
        external_gene_calls = rules.anvi_get_external_gene_calls_file.output.external_gene_calls,
    output:
        external_gene_calls_renamed = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-external_gene_calls_renamed.tsv")
    threads: M.T('rename_and_filter_external_gene_calls_file')
    script:
        "scripts/rename_external_gene_calls_file.py"
 

rule cat_external_gene_calls_file:
    """Cat all external_gene_calls files from all samples into one file"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "cat_external_gene_calls_file_{HMM}.log")
    input:
        external_gene_calls_renamed = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{sample_names}-{{HMM}}-external_gene_calls_renamed.tsv"), sample_names = M.names_list),
    output:
        external_gene_calls_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-external_gene_calls_all.tsv"),
    threads: M.T('cat_external_gene_calls_file')
    run:
        col_names = ["gene_callers_id", "contig", "start", "stop", "direction", "partial", "call_type", "source", "version", "aa_sequence"]
        with open(output.external_gene_calls_all, "a") as external_gene_calls_all:
                external_gene_calls_all.write('\t'.join(col_names) + "\n")
                for f in input.external_gene_calls_renamed:
                    file = open(f).read().splitlines(True)
                    external_gene_calls_all.writelines(file[1:])
                    # external_gene_calls_all.writelines(line + '\n' for line in file[1:]) # write without header


rule cluster_X_percent_sim_mmseqs:
    """
    Cluster SCG NT sequences with mmseqs to remove redundant SCG sequences which will prevent non-specific read recruitment
    and read recruitment dilution.
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "cluster_X_mmseqs_{HMM}.log")
    input: rules.combine_sequence_data.output.NT_all
    output: 
        fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-mmseqs_NR_rep_seq.fasta"),
        mmseqs_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-mmseqs_NR_cluster.tsv")
    params:
        output_prefix = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-mmseqs_NR"),
        mmseqs_tmp = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-tmp"),
        min_seq_id = M.get_param_value_from_config(['cluster_X_percent_sim_mmseqs', '--min-seq-id'])
    threads: M.T('cluster_X_percent_sim_mmseqs')
    run:
        shell(f"mmseqs easy-cluster {input} \
                                    {params.output_prefix} \
                                    {params.mmseqs_tmp} \
                                    --threads {threads} \
                                    --min-seq-id {params.min_seq_id} >> {log} 2>&1")


rule cluster_X_percent_sim_mmseqs_OTUs:
    """
    Cluster extracted proteins within percent ID parameter space provided by user. This will 
    help identify clustering thresholds for OTU like analyses.
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "cluster_X_mmseqs_{HMM}_{clustering_threshold}.log")
    input: rules.combine_sequence_data.output.NT_all
    output: 
        fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{clustering_threshold}", "{HMM}-{clustering_threshold}-mmseqs_NR_rep_seq.fasta"),
        mmseqs_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{clustering_threshold}", "{HMM}-{clustering_threshold}-mmseqs_NR_cluster.tsv")
    params:
        output_prefix = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{clustering_threshold}", "{HMM}-{clustering_threshold}-mmseqs_NR"),
        mmseqs_tmp = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{clustering_threshold}", "{HMM}-{clustering_threshold}-tmp"),
    threads: M.T('cluster_X_percent_sim_mmseqs')
    run:
        min_seq_id = M.clustering_threshold_dict[wildcards.clustering_threshold]
        shell("mmseqs easy-cluster {input} \
                                    {params.output_prefix} \
                                    {params.mmseqs_tmp} \
                                    --threads {threads} \
                                    --min-seq-id {min_seq_id} >> {log} 2>&1")


if M.cluster_representative_method == "cluster_rep_with_coverages":
    rule anvi_profile_blitz:
        """
        Choose a NT cluster rep based on read recruitment!

        The sequence with the most read recruitment from the input profiled assembly will be chosen as the cluster representative.
        """

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_profile_blitz-{sample_name}.log")
        input:
        output:
            os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-gene-coverages.txt"),
        params:
        threads: M.T('anvi_profile_blitz')
        run:
            contigsDB = os.path.join(M.contigsDB_name_path_dict[wildcards.sample_name])
            bam = os.path.join(M.contigsDB_name_bam_dict[wildcards.sample_name])

            # Run different HMM search depending on whether a HMM is internal or external because anvio
            shell("anvi-profile-blitz {bam} -c {contigsDB} --gene-mode --report-minimal -o {output} >> {log} 2>&1")


    rule cat_anvi_profile_blitz:
        """
        """

        version: 1.0
        # log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_profile_blitz-{sample_name}.log")
        input:
            expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-gene-coverages.txt"), sample_name = M.names_list),
        output:
            os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "gene-coverages.txt"),
        params:
        threads: M.T('anvi_profile_blitz')
        run:
            shell("echo -e 'gene_callers_id\tcontig\tsample\tlength\tdetection\tmean_cov' > {output}")
            shell("awk 'FNR>1' {input} >> {output}")


    rule pick_cluster_rep_with_coverage:
        """
        Pick a cluster rep with coverage values.
        """

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "pick_cluster_rep_with_coverage-{HMM}.log")
        input:
            mmseqs_cluster_rep_index = rules.cluster_X_percent_sim_mmseqs.output.mmseqs_cluster_rep_index,
            reformat_report = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-reformat-report-all.txt"),
            coverages = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "gene-coverages.txt") 
        output:
            coverage_reps = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-coverage-headers.txt"),
            coverage_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-coverage_cluster.tsv")
        params:
        run:
            # Bind anvi-profile-blitz data with cluster rep data and group_by cluster rep then find the cluster member with the highest coverage to pick new rep
            pd.set_option('expand_frame_repr', False)

            cluster_rep_index = pd.read_csv(input.mmseqs_cluster_rep_index, \
                                    sep="\t", \
                                    index_col=False, \
                                    names=["representative", "cluster_members"])

            reformat_report = pd.read_csv(input.reformat_report, \
                                            sep="\t", \
                                            index_col=False, \
                                            names=["new_header", "header"])

            bam = pd.read_csv(input.coverages, sep="\t", index_col=False)

            bam['primary_key'] = bam['contig'] + "_" + bam['gene_callers_id'].astype(str)
            reformat_report["gene_callers_id"] = reformat_report['header'].str.split("gene_callers_id:|\|start:", expand=True)[1].astype(str)
            reformat_report["contig"] = reformat_report['header'].str.split("contig:|\|gene_callers_id:", expand=True)[1].astype(str)
            reformat_report['primary_key'] = reformat_report['contig'] + "_" + reformat_report['gene_callers_id'].astype(str)
            
            df = pd.merge(reformat_report, bam, on='primary_key', how='inner')
            df2 = pd.merge(cluster_rep_index, df, left_on='cluster_members', right_on='new_header', how='inner')[["representative", "cluster_members", "mean_cov"]]

            def get_new_seed(df):
                new_seed = df[df['mean_cov'] == df['mean_cov'].max()]['cluster_members'].iloc[0]
                df['representative'] = [new_seed] * len(df)
                return df

            df3 = df2.groupby('representative').apply(get_new_seed)

            # export headers of representatives and cluster rep index
            df3[["representative"]].drop_duplicates().to_csv(output.coverage_reps, sep="\t", index=None, header=False)
            df3[["representative", "cluster_members"]].to_csv(output.coverage_cluster_rep_index, sep="\t", index=None, na_rep="NA")



    rule subset_AA_seqs_with_coverage_reps:
        """Subset AA sequences for the mmseqs cluster representatives"""

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "subset_AA_seqs_with_coverage_reps_{HMM}.log")
        input:
            fa = rules.combine_sequence_data.output.AA_all,
            coverage_reps = rules.pick_cluster_rep_with_coverage.output.coverage_reps,
        output:
            fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-AA_subset.fa"),
        threads: M.T('subset_AA_seqs_with_coverage_reps')
        run:
            shell("anvi-script-reformat-fasta {input.fa} -I {input.coverage_reps} -o {output.fasta} >> {log} 2>&1")


if M.cluster_representative_method == "mmseqs":
    rule subset_AA_seqs_with_mmseqs_reps:
        """Subset AA sequences for the mmseqs cluster representatives"""

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "subset_AA_seqs_with_mmseqs_reps_{HMM}.log")
        input:
            fa = rules.combine_sequence_data.output.AA_all,
            mmseqs_reps = rules.cluster_X_percent_sim_mmseqs.output.fasta,
        output:
            fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-AA_subset.fa"),
        params:
            headers = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-headers.tmp")
        threads: M.T('subset_AA_seqs_with_mmseqs_reps')
        run:
            shell("grep '>' {input.mmseqs_reps} | sed 's/>//g' > {params.headers}")
            shell("anvi-script-reformat-fasta {input.fa} -I {params.headers} -o {output.fasta} >> {log} 2>&1")


rule align_sequences:
    """MSA of AA sequences subset."""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "align_sequences_{HMM}.log")
    input: os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-AA_subset.fa"),
    output: os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}-aligned.fa"),
    params: additional_params = M.get_param_value_from_config(['align_sequences', 'additional_params'])
    threads: M.T('align_sequences')
    run:
        shell("muscle -in {input} -out {output} {params.additional_params} -verbose 2> {log}")


rule trim_alignment:
    """Trim MSA alignment"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "trim_alignment_{HMM}.log")
    input: rules.align_sequences.output
    output: os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_aligned_trimmed.fa")
    params:
        gt = M.get_param_value_from_config(['trim_alignment', '-gt']),
        gappyout = M.get_rule_param('trim_alignment', '-gappyout'),
        additional_params = M.get_param_value_from_config(['trim_alignment', 'additional_params'])
    threads: M.T('trim_alignment')
    run:
        shell('trimal -in {input} -out {output} {params.gappyout} {params.additional_params} 2> {log}')


rule remove_sequences_with_X_percent_gaps:
    """Removing sequences that have Z > X% gaps"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "remove_sequences_with_X_percent_gaps_{HMM}.log")
    input: rules.trim_alignment.output
    output: 
        fasta = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_aligned_trimmed_filtered.fa")
    params:
        seq_counts_tsv = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_gaps_counts"),
        max_percentage_gaps = M.get_param_value_from_config(['remove_sequences_with_X_percent_gaps', '--max-percentage-gaps'])
    threads: M.T('remove_sequences_with_X_percent_gaps')
	run:
		shell("anvi-script-reformat-fasta {input} -o {output.fasta} \
									              --max-percentage-gaps {params.max_percentage_gaps} \
									              --export-gap-counts-table {params.seq_counts_tsv} >> {log} 2>&1")


rule count_num_sequences_filtered:
    """Record the number of sequences filtered at each step of the workflow"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "count_num_sequences_filtered_{HMM}.log")
    input:
        step1 = rules.combine_sequence_data.output.NT_all,
        step2 = rules.cluster_X_percent_sim_mmseqs.output.fasta,
        step3 = rules.remove_sequences_with_X_percent_gaps.output.fasta,
        clustering_thresholds = expand(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{{HMM}}", "{clustering_threshold}", "{{HMM}}-{clustering_threshold}-mmseqs_NR_rep_seq.fasta"), clustering_threshold = M.clustering_param_space_list_strings),
    output: os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_MSA_STATS'], "{HMM}", "{HMM}_stats.tsv")
    threads: M.T('count_num_sequences_filtered')
    run:
        def count_num_sequences(fasta):
            num_seqs = 0
            for line in fasta:
                if line.startswith(">"):
                    num_seqs += 1
            return num_seqs

        input_files_list = [input.step1, input.step2, input.step3]
        num_seqs_list = [count_num_sequences(open(fasta)) for fasta in input_files_list]

        clustering_threshold_attributes_list = []
        for file in input.clustering_thresholds:
            path = file
            threshold = file.split("/")[3]
            with open(file) as fasta:
                num_seqs = count_num_sequences(fasta)
            clustering_threshold_attributes = [str(threshold), str(num_seqs), path]
            clustering_threshold_attributes_list.append(clustering_threshold_attributes)

        with open(output[0], 'w') as f:
            col_names = ["rule_name", "num_sequences_left", "rel_path"]
            step1 = ["combine_sequence_data", str(num_seqs_list[0]), input.step1]
            step2 = ["cluster_X_percent_sim_mmseqs", str(num_seqs_list[1]), input.step2]
            step3 = ["remove_sequences_with_X_percent_gaps", str(num_seqs_list[2]), input.step3]
            lines = [col_names, step1, step2, step3] + clustering_threshold_attributes_list 
            for line in lines:
                f.write('\t'.join(line) + '\n')


rule subset_DNA_reps_with_QCd_AA_reps_for_mapping:
    """Filter for SCG NT sequences what will be used for read recruitment later"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "subset_DNA_reps_with_QCd_AA_reps_for_mapping_{HMM}.log")
    input:
        fasta = rules.combine_sequence_data.output.NT_all,
        reps = rules.remove_sequences_with_X_percent_gaps.output
    output:
        NT_for_mapping = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-references_for_mapping_NT.fa"),
        headers = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_headers.tmp")
    threads: M.T('subset_DNA_reps_with_QCd_AA_reps_for_mapping')
    run:
        shell("grep '>' {input.reps} | sed 's/>//g' > {output.headers}")

        shell("anvi-script-reformat-fasta {input.fasta} -I {output.headers} -o {output.NT_for_mapping} >> {log} 2>&1")


rule subset_external_gene_calls_file_all:
    """
    Subset the concatenated exteral_gene_calls.txt for the final set of NT sequences for profiling.
    ALSO, once all external-gene-calls are concatenated, gene-callers-ids will be re-indexed so that 
    there is NO repeating values. 
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "subset_external_gene_calls_file_all_{HMM}.log")
    input:
        external_gene_calls_all = rules.cat_external_gene_calls_file.output.external_gene_calls_all,
        headers = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_headers.tmp")
    output:
        external_gene_calls_subset = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-external_gene_calls_subset.tsv"),
    threads: M.T('subset_external_gene_calls_file_all')
    script:
        "scripts/subset_external_gene_calls_file.py"


rule make_fasta_txt:
    """
    Format a fasta.txt with the filtered NT sequences that will be profiled 
    using the metagenomics workflow.
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "make_fasta_txt.log")
    input:
        expand(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-external_gene_calls_subset.tsv"), HMM = M.HMM_source_dict.keys()),
        expand(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-references_for_mapping_NT.fa"), HMM = M.HMM_source_dict.keys()),
    output:
        fasta_txt = os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW", "fasta.txt"),
    threads: M.T('make_fasta_txt')
    run:
        fastas = [os.path.join("..", "02_NR_FASTAS", r + "/" + r + '-references_for_mapping_NT.fa') for r in M.HMM_source_dict.keys()]
        external_gene_calls = [os.path.join("..", "02_NR_FASTAS", r + "/" + r + '-external_gene_calls_subset.tsv') for r in M.HMM_source_dict.keys()]

        list_of_strings = ['\t'.join(t) + '\n' for t in zip(M.HMM_source_dict.keys(), fastas, external_gene_calls)]

        shell('echo -e "name\tpath\texternal_gene_calls" > {output.fasta_txt}')
        shell('echo -n "%s" >> {output.fasta_txt}' % ''.join(list_of_strings))

if M.run_iqtree == True:
  rule iqtree:
      """Calculate a phylogenetic tree using iqtree"""

      version: 1.0
      log: os.path.join(dirs_dict['LOGS_DIR'], "iqtree_{HMM}.log")
      input: rules.remove_sequences_with_X_percent_gaps.output
      output: 
          tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.iqtree"),
          done = touch(os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}-tree.done"))
      params:
          outfile=os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}"),
          model = M.get_param_value_from_config(['iqtree', '-m']),
          additional_params = M.get_param_value_from_config(['iqtree', 'additional_params'])
      threads: M.T('iqtree')
      run:
          shell('iqtree -s {input} -nt AUTO -m {params.model} -pre {params.outfile}  -T AUTO {params.additional_params} >> {log} 2>&1')

elif M.run_fasttree == True:
  rule fasttree:
      """Want to go faster?? Then Fasttree"""

      version: 1.0
      log: os.path.join(dirs_dict['LOGS_DIR'], "fasttree_{HMM}.log")
      input: rules.remove_sequences_with_X_percent_gaps.output
      output:
          tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.nwk"),
          done = touch(os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}-tree.done"))
      params:
          additional_params = M.get_param_value_from_config(['fasttree', 'additional_params']),
          tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.nwk"),
      threads: M.T('fasttree')
      run:
        shell("FastTree -fastest {input} 2> {log} 1> {params.tree}")


rule rename_tree_tips:
    """Add "_split_00001" suffix to tree tips names to bind with profileDB"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "rename_tree_tips_{HMM}.log")
    input:
        tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}-tree.done"),
    output:
        done = os.path.join(dirs_dict['TREES'], "{HMM}_combined.done"),
        tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}_renamed.nwk"),
        fasta = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}_renamed.faa")
    params:
        fasttree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.nwk"),
        iqtree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.iqtree"),
        fasta = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_aligned_trimmed_filtered.fa")
    threads: M.T('rename_tree_tips')
    run:
        # rename tree tips 
        from ete3 import Tree

        def add_split_name_to_tree_tips(tree):

            t = Tree(tree)

            for leaf in t:
                leaf.name = leaf.name + "_split_00001"

            t.write(format = 1, outfile = output.tree)

            shell('touch {output}')
        
        if M.run_iqtree == True:
            add_split_name_to_tree_tips(params.iqtree)
        
        elif M.run_fasttree == True:
            add_split_name_to_tree_tips(params.fasttree)
        
        # rename fasta headers to match tree
        fasta = open(str(params.fasta))
        newfasta = open(output.fasta, "a")

        for line in fasta:
            if line.startswith('>'):
                newname = line.rstrip("\n") + "_split_00001\n"
                newfasta.write(newname)
            else:
                newfasta.write(line)

        fasta.close()
        newfasta.close()


def extract_misc_data(mmseqs_cluster_rep_index, final_sequences_headers, output):
    """
    Here we will extract misc data to add as layers to the interactive interface.
    e.g. cluster size, contigsDB type, etc. 
    """

    # Import data
    #------------
    cluster_rep_index = pd.read_csv(mmseqs_cluster_rep_index, sep="\t", index_col=False, names=["representative", "cluster_members"])

    final_sequences_headers = pd.read_csv(final_sequences_headers, sep="\t", index_col=False, names=["identifier"])
    
    # Clean data
    #------------
    
    # Detect if there is a genomic reference protein in cluster
    cluster_rep_index_dict = cluster_rep_index.groupby('representative')['cluster_members'].apply(list).to_dict()

    cluster_reps_with_genomic_references_list = []
    for seq in final_sequences_headers.iloc[:, 0].tolist():
        cluster_members_list = cluster_rep_index_dict[seq]
        for external_genome in M.external_genomes_names_list:
            check = any(external_genome in s for s in cluster_members_list)
            if check is True:
                cluster_reps_with_genomic_references_list.append(seq)

    # Count size of clusters
    df = cluster_rep_index.groupby('representative').apply(count_cluster_size)[['cluster_members', 'cluster_size']]

    # Make split names for anvi-interactive
    df['split_name'] = df['cluster_members'].astype(str) + '_split_00001' 

    # subset misc data to final set of proteins
    df = pd.merge(df, final_sequences_headers, left_on='cluster_members', right_on='identifier', how='inner')

    df['genomic_seq_in_cluster'] = np.where(df['cluster_members'].isin(cluster_reps_with_genomic_references_list), 'yes', 'no')

    # Determine contigsDB type: metagenome or genomes
    # FIXME: This will need to be changed in the future to accomidate SAGs, MAGs, and other genomic sources
    # If metagenome_name is in external_genomes_names_list then it's a genome
    contigsDB_type_dict = {}
    for name in list(df.cluster_members):
        if any(x in name for x in M.external_genomes_names_list):
            contigsDB_type_dict[name] = "genome"
        else:
            contigsDB_type_dict[name] = "metagenome"
        
    df["contig_db_type"] = df.cluster_members.apply(lambda name: contigsDB_type_dict[name])

    # grab the final columns
    df = df[['split_name', 'contig_db_type', 'genomic_seq_in_cluster', 'cluster_size']]

    # Export
    #-------
    df.to_csv(output, \
              sep="\t", \
              index=None, \
              na_rep="NA")

def count_cluster_size(group):
    c = group['cluster_members'].count()
    group['cluster_size'] = c

    return group


rule make_misc_data:
    """Make misc data file for sequences"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "add_contigsDB_type_{HMM}.log")
    input:
        final_list_of_sequences_for_mapping_headers = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_headers.tmp")
    output:
        misc_data_final = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_misc.tsv")
    params:
        mmseqs_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-mmseqs_NR_cluster.tsv"),
        coverage_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-coverage_cluster.tsv")
    threads: M.T('add_misc_data_to_taxonomy')
    run:
        """Here we determine the origin of each SCG (which kind of contigsDB): metagenome, isolate genome, etc."""
        if M.cluster_representative_method == "mmseqs":
            extract_misc_data(mmseqs_cluster_rep_index = params.mmseqs_cluster_rep_index,
                              final_sequences_headers = input.final_list_of_sequences_for_mapping_headers,
                              output = output.misc_data_final)
        if M.cluster_representative_method == "cluster_rep_with_coverages":
            extract_misc_data(mmseqs_cluster_rep_index = params.coverage_cluster_rep_index,
                              final_sequences_headers = input.final_list_of_sequences_for_mapping_headers,
                              output = output.misc_data_final)



rule anvi_scg_taxonomy:
    """Run anvi-estimate-SCG-taxonomy and import the resulting taxonomy misc data to profileDB for internal HMMs only"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_scg_taxonomy_{HMM}.log")
    input:
        misc_data = rules.make_misc_data.output.misc_data_final,
        reformat_file = rules.combine_sequence_data.output.reformat_report_all,
    params:
        combined_genomes_txt = "combined_genomes.txt",
        taxonomy = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_estimate_scg_taxonomy_results"),
        taxonomy_long = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_estimate_scg_taxonomy_results-RAW-LONG-FORMAT.txt"),
        final_list_of_sequences_for_mapping_headers = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_headers.tmp"),
        tax_data_final = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_scg_taxonomy_data.tsv"),
    output: 
        # tax_data_final = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_scg_taxonomy_data.tsv"),
        done = touch(os.path.join("ECOPHYLO_WORKFLOW", "{HMM}_anvi_estimate_scg_taxonomy_for_SCGs.done"))
    threads: M.T('anvi_scg_taxonomy')
    run:
        HMM_source = M.HMM_source_dict[wildcards.HMM]
        
        if HMM_source in M.internal_HMM_sources:

            shell("anvi-estimate-scg-taxonomy -M {params.combined_genomes_txt} \
                                              --metagenome-mode \
                                              --scg-name-for-metagenome-mode {wildcards.HMM} \
                                              -T {threads} \
                                              --raw-output \
                                              -O {params.taxonomy} &> {log}")
            # Import data
            #------------
            scg_taxonomy = pd.read_csv(params.taxonomy_long, \
                                       sep="\t", \
                                       index_col=False)
                                            
            reformat_report = pd.read_csv(input.reformat_file, \
                                          sep="\t", \
                                          index_col=False, \
                                          names=["new_header", "header"])

            final_sequences_headers = pd.read_csv(params.final_list_of_sequences_for_mapping_headers, \
                                                sep="\t", \
                                                index_col=False, \
                                                names=["identifier"])
            
            # Clean Data
            #-----------
            reformat_report = pd.merge(reformat_report, final_sequences_headers, left_on='new_header', right_on='identifier', how='inner')
            reformat_report["gene_callers_id"] = reformat_report['header'].str.split("gene_callers_id:|\|start:", expand=True)[1].astype(str)
            reformat_report["new_header_tmp"] = reformat_report['new_header'].str.rsplit('_', 1).str[0]
            reformat_report["identifier"] = reformat_report["new_header_tmp"] + "_" + reformat_report["gene_callers_id"] 
            scg_taxonomy["identifier"] =  scg_taxonomy["metagenome_name"] + "_" +  scg_taxonomy["gene_name"] + "_" +  scg_taxonomy["gene_callers_id"].astype(str)
            scg_taxonomy = scg_taxonomy.merge(reformat_report, on='identifier', how="inner")
            scg_taxonomy['split_name'] = scg_taxonomy['new_header'].astype(str) + '_split_00001' 
            scg_taxonomy = scg_taxonomy[["split_name", "identifier", "percent_identity", "t_domain", "t_phylum", "t_class", "t_order", "t_family", "t_genus", "t_species"]]

            # Export
            #-------
            scg_taxonomy.to_csv(params.tax_data_final, \
                                sep="\t", \
                                index=None, \
                                na_rep="NA")

        else:
            pass


if M.samples_txt_file:
    # PROFILE-MODE with read recruitment
    include: "rules/profile_mode.smk"
else:
    # TREE-MODE
    include: "rules/tree_mode.smk"