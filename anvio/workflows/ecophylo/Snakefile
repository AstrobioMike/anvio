# -*- coding: utf-8
import os
import json
import os.path
import argparse

import numpy as np
import pandas as pd

import anvio
import anvio.utils as u
import anvio.workflows as w

from Bio import SeqIO
from anvio.dbops import ContigsDatabase
from anvio.errors import ConfigError
from anvio.workflows.ecophylo import EcoPhyloWorkflow

__author__ = "Matthew S. Schechter"
__copyright__ = "Copyright 2017, The anvio Project"
__credits__ = []
__license__ = "GPL 3.0"
__version__ = anvio.__version__
__maintainer__ = "Matthew S. Schechter"
__email__ = "mschechter@uchicago.edu"


M = EcoPhyloWorkflow(argparse.Namespace(config=config))
M.init()

dirs_dict = M.dirs_dict

rule ECOPHYLO_WORKFLOW_target_rule:
    input: M.target_files


rule anvi_run_hmms_hmmsearch:
    """
    Run hmmsearch with input HMMs to get domtblout.
    If incoming contigsDBs do not have `scg_taxonomy_was_run` then run anvi-run-scg-taxonomy 
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_run_hmms_hmmsearch-{sample_name}-{HMM}.log")
    input:
    output:
        done = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-{HMM}-contigs-hmmsearch.done"),
    params:
        hmm_source = M.get_param_value_from_config(['anvi_run_hmms_hmmsearch', '--installed-hmm-profile']),
        additional_params = M.get_param_value_from_config(['trim_alignment', 'additional_params'])
    threads: M.T('anvi_run_hmms_hmmsearch')
    run:
        contigsDB = os.path.join(M.contigsDB_name_path_dict[wildcards.sample_name])
        HMM_dir = os.path.join(M.HMM_path_dict[wildcards.HMM])
        HMM_source = M.HMM_source_dict[wildcards.HMM]
        hmmer_output_dir = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}-{HMM_source}-dom_hmmsearch")
        domtblout = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}-{HMM_source}-dom_hmmsearch/hmm.domtable")

        # Run different HMM search depending on whether a HMM is internal or external because anvio
        if HMM_source in M.internal_HMM_sources:
            if not os.path.exists(domtblout):
                print(f"Running internal HMM dataset: {HMM_source}")
                shell("anvi-run-hmms -c {contigsDB} \
                                    --hmmer-program hmmsearch \
                                    --hmmer-output-dir {hmmer_output_dir} \
                                    -I {HMM_source} \
                                    --domain-hits-table \
                                    --just-do-it \
                                    -T {threads} 2> {log}")
            else:
                pass

            # Load contigsDB
            contigs_db = ContigsDatabase(contigsDB)
            # Check if anvi-run-scg-taxonomy has been run
            scg_taxonomy_was_run_value = contigs_db.meta['scg_taxonomy_was_run']

            if scg_taxonomy_was_run_value != 1:
                print(f"Running anvi-run-scg-taxonomy on {contigsDB} since the {wildcards.HMM} is in anvio's collection of SCGs")
                shell("anvi-run-scg-taxonomy -c {contigsDB} --num-threads {threads} {params.additional_params}")
        else:
            if not os.path.exists(domtblout):
                print(f"Running external HMM dataset: {wildcards.HMM}")
                shell("anvi-run-hmms -c {contigsDB} \
                                    --hmmer-program hmmsearch \
                                    --hmm-profile-dir {HMM_dir} \
                                    --hmmer-output-dir {hmmer_output_dir} \
                                    --domain-hits-table \
                                    --just-do-it \
                                    -T {threads} 2> {log}")
            else:
                pass
        
        shell("touch {output.done}")


rule filter_hmm_hits_by_query_coverage:
    """
    Filter hmm_hits table in the contigsDB by query coverage using domtblout.
    This will remove any sketchy sequences that were recruited by the HMM.
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "filter_hmm_hits_by_query_coverage-{sample_name}-{HMM}.log")
    input:
        done = rules.anvi_run_hmms_hmmsearch.output.done,
    output:
        done = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-{HMM}-DB_filtered.done")
    params:
        query_coverage = M.get_param_value_from_config(['filter_hmm_hits_by_query_coverage', '--query-coverage']),
        additional_params = M.get_param_value_from_config(['filter_hmm_hits_by_query_coverage', 'additional_params'])
    threads: M.T('filter_hmm_hits_by_query_coverage')
    run:
        contigsDB = os.path.join(M.contigsDB_name_path_dict[wildcards.sample_name])
        HMM_source = M.HMM_source_dict[wildcards.HMM]
        HMM_dir = os.path.join(M.HMM_path_dict[wildcards.HMM])
        HMM_source = M.HMM_source_dict[wildcards.HMM]
        domtblout = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}-{HMM_source}-dom_hmmsearch/hmm.domtable")
        hmmer_output_dir = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], f"{wildcards.sample_name}-{HMM_source}-dom_hmmsearch")

        # Check if anvi-run-scg-taxonomy and/or anvi-script-filter-hmm-hits-table has been run already
        contigs_db = ContigsDatabase(contigsDB)
        try:
            HMM_dom_filter_sources = contigs_db.meta['HMM_dom_filter_sources']
            HMM_dom_filter_target_coverage = contigs_db.meta['HMM_dom_filter_target_coverage']
            HMM_dom_filter_query_coverage = contigs_db.meta['HMM_dom_filter_query_coverage']

            HMM_dom_filter_sources_list = HMM_dom_filter_sources.split(",")
            HMM_dom_filter_target_coverage_list = HMM_dom_filter_target_coverage.split(",")
            HMM_dom_filter_query_coverage_list = HMM_dom_filter_query_coverage.split(",")

            source_domain_filter_values = list(zip(HMM_dom_filter_sources_list, HMM_dom_filter_target_coverage_list, HMM_dom_filter_query_coverage_list))

            domain_filter_values_dict = {}
            for item in source_domain_filter_values:
                domain_filter_values_dict[item[0]] = item
        except:
            domain_filter_values_dict = {}

        contigs_db.disconnect()

        if HMM_source in M.internal_HMM_sources:
            if HMM_source in domain_filter_values_dict.keys():
                if float(params.query_coverage) > float(domain_filter_values_dict[HMM_source][2]):
                    shell(f"anvi-script-filter-hmm-hits-table -c {contigsDB} \
                                                            --domain-hits-table {domtblout} \
                                                            --hmm-source {HMM_source} \
                                                            --query-coverage {params.query_coverage} \
                                                            {params.additional_params} 2> {log}")
                else:
                    print(f"The HMM source {HMM_source} has already been filtered with more stringent query coverage value: {domain_filter_values_dict[HMM_source][2]}, skipping filter_hmm_hits_by_query_coverage!")
                    pass
            else:
                shell(f"anvi-script-filter-hmm-hits-table -c {contigsDB} \
                                                        --domain-hits-table {domtblout} \
                                                        --hmm-source {HMM_source} \
                                                        --query-coverage {params.query_coverage} \
                                                        {params.additional_params} 2> {log}")
        else:
            if HMM_source in domain_filter_values_dict.keys():
                if float(params.query_coverage) > float(domain_filter_values_dict[HMM_source][2]):
                    shell(f"anvi-script-filter-hmm-hits-table -c {contigsDB} \
                                                            --domain-hits-table {domtblout} \
                                                            --hmm-profile-dir {HMM_dir} \
                                                            --hmm-source {HMM_source} \
                                                            --query-coverage {params.query_coverage} \
                                                            {params.additional_params} 2> {log}")
                else:
                    print(f"The HMM source {HMM_source} has already been filtered with more stringent query coverage value: {domain_filter_values_dict[HMM_source][2]}, skipping filter_hmm_hits_by_query_coverage!")
                    pass
                
        shell('touch {output.done}')


rule anvi_get_sequences_for_hmm_hits:
    """
    Extract all AA and NT sequences that were recruited by the HMM

    Snakemake needs to pass through this rule regardless if it's able to extract a 
    target protein from it e.g. I still want the workflow to cat all of the target proteins
    even if a few of the MAGs DO NOT have the target protein. I think one solution would be 
    to touch a done file immediately so that even if the rule fails the file will be there and the
    workflow can move on. This is sketchy though because if the rule fails for another reason
    then we wont know what happened unless the user checks the individual logs
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_get_sequences_for_hmm_hits-{sample_name}-{HMM}.log"),
    input:
        done_file = rules.filter_hmm_hits_by_query_coverage.output.done,
    output:
        done = touch(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits.faa.done")),
        faa = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits.faa"),
        fna = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits.fna"),
    threads: M.T('anvi_get_sequences_for_hmm_hits')
    run:
        HMM_source = M.HMM_source_dict[wildcards.HMM]
        contigsDB = os.path.join(M.contigsDB_name_path_dict[wildcards.sample_name])

        if HMM_source in M.internal_HMM_sources:
            shell("anvi-get-sequences-for-hmm-hits -c {contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                --gene-names {wildcards.HMM} \
                                                --get-aa-sequences \
                                                -o {output.faa} --just-do-it 2> {log}")

            shell("anvi-get-sequences-for-hmm-hits -c {contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                --gene-names {wildcards.HMM} \
                                                -o {output.fna} --just-do-it 2> {log}")
        else:
            shell("anvi-get-sequences-for-hmm-hits -c {contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                --get-aa-sequences \
                                                -o {output.faa} --just-do-it 2> {log}")

            shell("anvi-get-sequences-for-hmm-hits -c {contigsDB} \
                                                --hmm-sources  {HMM_source} \
                                                -o {output.fna} --just-do-it 2> {log}")


rule simplify_names_from_hmm_hits:
    """Clean up fasta headers for tree calculation"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "simplify_names_from_hmm_hits-{sample_name}_{HMM}.log"),
    input: 
        NT = rules.anvi_get_sequences_for_hmm_hits.output.fna,
        AA = rules.anvi_get_sequences_for_hmm_hits.output.faa
    output:
        fasta_NT = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits_renamed.fna"),
        fasta_AA = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-hmm_hits_renamed.faa"),
        report_file_NT = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-reformat_report_nt.txt"),
        report_file_AA = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-reformat_report_AA.txt")
    params:

    threads: M.T('simplify_names_from_hmm_hits')
    run:
        prefix_list = [wildcards.sample_name, wildcards.HMM]
        prefix = "_".join(prefix_list) 

        shell(f"anvi-script-reformat-fasta {input.NT} \
                              --simplify-names \
                              --prefix {prefix} \
                              --report-file {output.report_file_NT} \
                              -o {output.fasta_NT} >> {log} 2>&1")

        shell(f"anvi-script-reformat-fasta {input.AA} \
                              --simplify-names \
                              --prefix {prefix} \
                              --report-file {output.report_file_AA} \
                              -o {output.fasta_AA} >> {log} 2>&1")


rule cat_sequences_to_one_fasta:
    """
    Cat all sequences and reformat_files from seperate metagenomes, genomes, SAGs, or MAGs into one fasta
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "cat_sequences_to_one_fasta_{HMM}.log")
    input:
        NT = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{sample_names}-{{HMM}}-hmm_hits_renamed.fna"), sample_names = M.names_list),
        AA = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{sample_names}-{{HMM}}-hmm_hits_renamed.faa"), sample_names = M.names_list),
        reformat_report_list = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{sample_names}-{{HMM}}-reformat_report_AA.txt"), sample_names = M.names_list),
    output:
        NT_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-all.fna"),
        AA_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-all.faa"),
        reformat_report_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-reformat-report-all.txt"),
    threads: M.T('cat_sequences_to_one_fasta')
    run:
        shell(f"cat {input.NT} >> {output.NT_all}")

        shell(f"cat {input.AA} >> {output.AA_all}")

        shell(f"cat {input.reformat_report_list} >> {output.reformat_report_all}")


rule anvi_get_external_gene_calls_file:
    """Extract external_gene_calls table from all samples"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "get_external_gene_calls_file_{sample_name}_{HMM}.log")
    input:
        done = rules.anvi_get_sequences_for_hmm_hits.output.done,
    output:
        external_gene_calls = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-external_gene_calls.tsv")
    params:
        fasta = temp(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-orfs.fna")),
    threads: M.T('anvi_get_external_gene_calls_file')
    run:
        contigsDB = os.path.join(M.contigsDB_name_path_dict[wildcards.sample_name])
        shell('anvi-get-sequences-for-gene-calls -c {contigsDB} \
                                                 --external-gene-calls {output.external_gene_calls} \
                                                 -o {params.fasta} 2> {log}')


rule rename_and_filter_external_gene_calls_file:
    """
    Create primary key from external-gene-calls.txt to join with references sequences to make
    a contigsDB for the metagenomics workflow
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "rename_and_filter_external_gene_calls_file_{sample_name}_{HMM}.log")
    input:
        reformat_file = rules.simplify_names_from_hmm_hits.output.report_file_AA,
        external_gene_calls = rules.anvi_get_external_gene_calls_file.output.external_gene_calls,
    output:
        external_gene_calls_renamed = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}", "{sample_name}-{HMM}-external_gene_calls_renamed.tsv")
    threads: M.T('rename_and_filter_external_gene_calls_file')
    script:
        "scripts/rename_external_gene_calls_file.py"
 

rule cat_external_gene_calls_file:
    """Cat all external_gene_calls files from all samples into one file"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "cat_external_gene_calls_file_{HMM}.log")
    input:
        external_gene_calls_renamed = expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_names}", "{sample_names}-{{HMM}}-external_gene_calls_renamed.tsv"), sample_names = M.names_list),
    output:
        external_gene_calls_all = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-external_gene_calls_all.tsv"),
    threads: M.T('cat_external_gene_calls_file')
    run:
        shell(f"cat {input.external_gene_calls_renamed} >> {output.external_gene_calls_all} 2> {log}")


rule cluster_X_percent_sim_mmseqs:
    """
    Cluster SCG NT sequences with mmseqs to remove redundant SCG sequences which will prevent non-specific read recruitment
    and read recruitment dilution.
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "cluster_X_mmseqs_{HMM}.log")
    input: rules.cat_sequences_to_one_fasta.output.NT_all
    output: 
        fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-mmseqs_NR_rep_seq.fasta"),
        mmseqs_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-mmseqs_NR_cluster.tsv")
    params:
        output_prefix = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-mmseqs_NR"),
        mmseqs_tmp = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-tmp"),
        min_seq_id = M.get_param_value_from_config(['cluster_X_percent_sim_mmseqs', '--min-seq-id'])
    threads: M.T('cluster_X_percent_sim_mmseqs')
    run:
        shell(f"mmseqs easy-cluster {input} \
                                    {params.output_prefix} \
                                    {params.mmseqs_tmp} \
                                    --threads {threads} \
                                    --min-seq-id {params.min_seq_id} >> {log} 2>&1")

if M.cluster_representative_method == "cluster_rep_with_coverages":
    rule anvi_profile_blitz:
        """
        Choose a NT cluster rep based on read recruitment!

        The sequence with the most read recruitment from the input profiled assembly will be chosen as the cluster representative.
        """

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_profile_blitz-{sample_name}.log")
        input:
        output:
            os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-gene-coverages.txt"),
        params:
        threads: M.T('anvi_profile_blitz')
        run:
            contigsDB = os.path.join(M.contigsDB_name_path_dict[wildcards.sample_name])
            bam = os.path.join(M.contigsDB_name_bam_dict[wildcards.sample_name])

            # Run different HMM search depending on whether a HMM is internal or external because anvio
            shell("anvi-profile-blitz {bam} -c {contigsDB} --gene-mode --report-minimal -o {output}")


    rule cat_anvi_profile_blitz:
        """
        """

        version: 1.0
        # log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_profile_blitz-{sample_name}.log")
        input:
            expand(os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "{sample_name}-gene-coverages.txt"), sample_name = M.names_list),
        output:
            os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "gene-coverages.txt"),
        params:
        threads: M.T('anvi_profile_blitz')
        run:
            shell("echo -e 'gene_callers_id\tcontig\tsample\tlength\tdetection\tmean_cov' > {output}")
            shell("awk 'FNR>1' {input} >> {output}")


    rule pick_cluster_rep_with_coverage:
        """
        Pick a cluster rep with coverage values.
        """

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "pick_cluster_rep_with_coverage-{HMM}.log")
        input:
            mmseqs_cluster_rep_index = rules.cluster_X_percent_sim_mmseqs.output.mmseqs_cluster_rep_index,
            reformat_report = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-reformat-report-all.txt"),
            coverages = os.path.join(dirs_dict['EXTRACTED_RIBO_PROTEINS_DIR'], "gene-coverages.txt") 
        output:
            coverage_reps = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-coverage-headers.txt"),
            coverage_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-coverage_cluster.tsv")
        params:
        run:
            # Bind anvi-profile-blitz data with cluster rep data and group_by cluster rep then find the cluster member with the highest coverage to pick new rep
            pd.set_option('expand_frame_repr', False)

            cluster_rep_index = pd.read_csv(input.mmseqs_cluster_rep_index, \
                                    sep="\t", \
                                    index_col=False, \
                                    names=["representative", "cluster_members"])

            reformat_report = pd.read_csv(input.reformat_report, \
                                            sep="\t", \
                                            index_col=False, \
                                            names=["new_header", "header"])

            bam = pd.read_csv(input.coverages, sep="\t", index_col=False)

            bam['primary_key'] = bam['contig'] + "_" + bam['gene_callers_id'].astype(str)
            reformat_report["gene_callers_id"] = reformat_report['header'].str.split("gene_callers_id:|\|start:", expand=True)[1].astype(str)
            reformat_report["contig"] = reformat_report['header'].str.split("contig:|\|gene_callers_id:", expand=True)[1].astype(str)
            reformat_report['primary_key'] = reformat_report['contig'] + "_" + reformat_report['gene_callers_id'].astype(str)
            
            df = pd.merge(reformat_report, bam, on='primary_key', how='inner')
            df2 = pd.merge(cluster_rep_index, df, left_on='cluster_members', right_on='new_header', how='inner')[["representative", "cluster_members", "mean_cov"]]

            def get_new_seed(df):
                new_seed = df[df['mean_cov'] == df['mean_cov'].max()]['cluster_members'].iloc[0]
                df['representative'] = [new_seed] * len(df)
                return df

            df3 = df2.groupby('representative').apply(get_new_seed)

            # export headers of representatives and cluster rep index
            df3[["representative"]].drop_duplicates().to_csv(output.coverage_reps, sep="\t", index=None, header=False)
            df3[["representative", "cluster_members"]].to_csv(output.coverage_cluster_rep_index, sep="\t", index=None, na_rep="NA")



    rule subset_AA_seqs_with_coverage_reps:
        """Subset AA sequences for the mmseqs cluster representatives"""

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "subset_AA_seqs_with_coverage_reps_{HMM}.log")
        input:
            fa = rules.cat_sequences_to_one_fasta.output.AA_all,
            coverage_reps = rules.pick_cluster_rep_with_coverage.output.coverage_reps,
        output:
            fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-AA_subset.fa"),
        threads: M.T('subset_AA_seqs_with_coverage_reps')
        run:
            shell("anvi-script-reformat-fasta {input.fa} -I {input.coverage_reps} -o {output.fasta} >> {log} 2>&1")


if M.cluster_representative_method == "mmseqs":
    rule subset_AA_seqs_with_mmseqs_reps:
        """Subset AA sequences for the mmseqs cluster representatives"""

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "subset_AA_seqs_with_mmseqs_reps_{HMM}.log")
        input:
            fa = rules.cat_sequences_to_one_fasta.output.AA_all,
            mmseqs_reps = rules.cluster_X_percent_sim_mmseqs.output.fasta,
        output:
            fasta = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-AA_subset.fa"),
        params:
            headers = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-headers.tmp")
        threads: M.T('subset_AA_seqs_with_mmseqs_reps')
        run:
            shell("grep '>' {input.mmseqs_reps} | sed 's/>//g' > {params.headers}")
            shell("anvi-script-reformat-fasta {input.fa} -I {params.headers} -o {output.fasta} >> {log} 2>&1")


rule align_sequences:
    """MSA of AA sequences subset."""
    
    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "align_sequences_{HMM}.log")
	input: os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-AA_subset.fa")  
	output: os.path.join(dirs_dict['MSA'], "{HMM}-aligned.fa")
	threads: M.T('align_sequences')
	shell:
		"muscle -in {input} -out {output} -maxiters 3 -verbose 2> {log}"


rule trim_alignment:
    """Trim MSA alignment"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "trim_alignment_{HMM}.log")
    input: rules.align_sequences.output
    output: os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_aligned_trimmed.fa")
    params:
        gt = M.get_param_value_from_config(['trim_alignment', '-gt']),
        gappyout = M.get_rule_param('trim_alignment', '-gappyout'),
        additional_params = M.get_param_value_from_config(['trim_alignment', 'additional_params'])
    threads: M.T('trim_alignment')
    shell:
        'trimal -in {input} \
                -out {output} \
                {params.gappyout} \
                {params.additional_params} 2> {log}'


rule remove_sequences_with_X_percent_gaps:
    """Removing sequences that have Z > X% gaps"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "remove_sequences_with_X_percent_gaps_{HMM}.log")
    input: rules.trim_alignment.output
    output: os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_aligned_trimmed_filtered.fa")
    params:
        seq_counts_tsv = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_gaps_counts"),
        max_percentage_gaps = M.get_param_value_from_config(['remove_sequences_with_X_percent_gaps', '--max-percentage-gaps'])
    threads: M.T('remove_sequences_with_X_percent_gaps')
	shell:
		"anvi-script-reformat-fasta {input} \
									-o {output} \
									--max-percentage-gaps {params.max_percentage_gaps} \
									--export-gap-counts-table {params.seq_counts_tsv} >> {log} 2>&1"


rule count_num_sequences_filtered:
    """Record the number of sequences filtered at each step of the workflow"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "count_num_sequences_filtered_{HMM}.log")
    input:
        step1 = rules.cat_sequences_to_one_fasta.output.NT_all,
        step2 = rules.cluster_X_percent_sim_mmseqs.output,
        step3 = rules.remove_sequences_with_X_percent_gaps.output
    output: os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_MSA_STATS'], "{HMM}", "{HMM}_stats.tsv")
    threads: M.T('count_num_sequences_filtered')
    shell:
        """
        # How many SCGs (e.g. Ribosomal_L16) did we recruit across all input data (genomes and metagenomes)?
        step1=$(grep -c '>' {input.step1})
        echo -e "num_input_seqs\t$step1\t{input.step1}\n" > {output}

        # How many SCGs do we have after we cluster them at the nt level and pick representatives?
        step2=$(grep -c '>' {input.step2})
        echo -e "cluster_X_percent_sim_mmseqs\t$step2\t{input.step2}\n" >> {output}

        # How many seqs are in the NR set?
        step3=$(grep -c '>' {input.step3})
        echo -e "remove_sequences_with_X_percent_gaps\t$step3\t{input.step3}\n" >> {output}

        # Add column names here
        echo -e "Rule_name\tNum_sequences\trel_path\n$(cat {output})" > {output}
        """


rule subset_DNA_reps_with_QCd_AA_reps_for_mapping:
    """Filter for SCG NT sequences what will be used for read recruitment later"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "subset_DNA_reps_with_QCd_AA_reps_for_mapping_{HMM}.log")
    input:
        fasta = rules.cat_sequences_to_one_fasta.output.NT_all,
        reps = rules.remove_sequences_with_X_percent_gaps.output
    output:
        NT_for_mapping = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-references_for_mapping_NT.fa"),
        headers = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_headers.tmp")
    threads: M.T('subset_DNA_reps_with_QCd_AA_reps_for_mapping')
    run:
        shell("grep '>' {input.reps} | sed 's/>//g' > {output.headers}")

        shell("anvi-script-reformat-fasta {input.fasta} -I {output.headers} -o {output.NT_for_mapping} >> {log} 2>&1")


rule subset_external_gene_calls_file_all:
    """
    Subset the concatenated exteral_gene_calls.txt for the final set of NT sequences for profiling.
    ALSO, once all external-gene-calls are concatenated, gene-callers-ids will be re-indexed so that 
    there is NO repeating values. 
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "subset_external_gene_calls_file_all_{HMM}.log")
    input:
        external_gene_calls_all = rules.cat_external_gene_calls_file.output.external_gene_calls_all,
        headers = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_headers.tmp")
    output:
        external_gene_calls_subset = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-external_gene_calls_subset.tsv"),
    threads: M.T('subset_external_gene_calls_file_all')
    script:
        "scripts/subset_external_gene_calls_file.py"


rule make_fasta_txt:
    """
    Format a fasta.txt with the filtered NT sequences that will be profiled 
    using the metagenomics workflow.
    """

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "make_fasta_txt.log")
    input:
        expand(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-external_gene_calls_subset.tsv"), HMM = M.HMM_source_dict.keys()),
        expand(os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-references_for_mapping_NT.fa"), HMM = M.HMM_source_dict.keys()),
    output:
        fasta_txt = os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW", "fasta.txt"),
    threads: M.T('make_fasta_txt')
    run:
        fastas = [os.path.join("..", "02_NR_FASTAS", r + "/" + r + '-references_for_mapping_NT.fa') for r in M.HMM_source_dict.keys()]
        external_gene_calls = [os.path.join("..", "02_NR_FASTAS", r + "/" + r + '-external_gene_calls_subset.tsv') for r in M.HMM_source_dict.keys()]

        list_of_strings = ['\t'.join(t) + '\n' for t in zip(M.HMM_source_dict.keys(), fastas, external_gene_calls)]

        shell('echo -e "name\tpath\texternal_gene_calls" > {output.fasta_txt}')
        shell('echo -n "%s" >> {output.fasta_txt}' % ''.join(list_of_strings))

if M.run_iqtree == True:
  rule iqtree:
      """Calculate a phylogenetic tree using iqtree"""

      version: 1.0
      log: os.path.join(dirs_dict['LOGS_DIR'], "iqtree_{HMM}.log")
      input: rules.remove_sequences_with_X_percent_gaps.output
      output: 
          tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.iqtree"),
          done = touch(os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}-tree.done"))
      params:
          outfile=os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}"),
          model = M.get_param_value_from_config(['iqtree', '-m']),
          additional_params = M.get_param_value_from_config(['iqtree', 'additional_params'])
      threads: M.T('iqtree')
      shell:
          "iqtree -s {input} \
                  -nt AUTO \
                  -m {params.model} \
                  -pre {params.outfile} \
                  -T AUTO \
                  {params.additional_params} >> {log} 2>&1"

elif M.run_fasttree == True:
  rule fasttree:
      """Want to go faster?? Then Fasttree"""

      version: 1.0
      log: os.path.join(dirs_dict['LOGS_DIR'], "fasttree_{HMM}.log")
      input: rules.remove_sequences_with_X_percent_gaps.output
      output:
          tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.nwk"),
          done = touch(os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}-tree.done"))
      params:
          additional_params = M.get_param_value_from_config(['fasttree', 'additional_params']),
          tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.nwk"),
      threads: M.T('fasttree')
      run:
        shell("FastTree -fastest {input} 2> {log} 1> {params.tree}")


rule rename_tree_tips:
    """Add "_split_00001" suffix to tree tips names to bind with profileDB"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "rename_tree_tips_{HMM}.log")
    input:
        tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}-tree.done")
    output:
        done = os.path.join(dirs_dict['TREES'], "{HMM}_combined.done"),
        tree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}_renamed.nwk")
    params:
        fasttree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.nwk"),
        iqtree = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}.iqtree")
    threads: M.T('rename_tree_tips')
    run:
        from ete3 import Tree

        def add_split_name_to_tree_tips(tree):

            t = Tree(tree)

            for leaf in t:
                leaf.name = leaf.name + "_split_00001"

            t.write(format = 1, outfile = output.tree)

            shell('touch {output}')
        
        if M.run_iqtree == True:
            add_split_name_to_tree_tips(params.iqtree)
        
        elif M.run_fasttree == True:
            add_split_name_to_tree_tips(params.fasttree)


def extract_misc_data(mmseqs_cluster_rep_index, final_sequences_headers, output):
    """
    Here we will extract misc data to add as layers to the interactive interface.
    e.g. cluster size, contigsDB type, etc. 
    """

    # Import data
    #------------
    cluster_rep_index = pd.read_csv(mmseqs_cluster_rep_index, sep="\t", index_col=False, names=["representative", "cluster_members"])

    final_sequences_headers = pd.read_csv(final_sequences_headers, sep="\t", index_col=False, names=["identifier"])
    
    # Clean data
    #------------
    
    # Detect if there is a genomic reference protein in cluster
    cluster_rep_index_dict = cluster_rep_index.groupby('representative')['cluster_members'].apply(list).to_dict()

    cluster_reps_with_genomic_references_list = []
    for seq in final_sequences_headers.iloc[:, 0].tolist():
        cluster_members_list = cluster_rep_index_dict[seq]
        for external_genome in M.external_genomes_names_list:
            check = any(external_genome in s for s in cluster_members_list)
            if check is True:
                cluster_reps_with_genomic_references_list.append(seq)

    # Count size of clusters
    df = cluster_rep_index.groupby('representative').apply(count_cluster_size)[['cluster_members', 'cluster_size']]

    # Make split names for anvi-interactive
    df['split_name'] = df['cluster_members'].astype(str) + '_split_00001' 

    # subset misc data to final set of proteins
    df = pd.merge(df, final_sequences_headers, left_on='cluster_members', right_on='identifier', how='inner')

    df['has_genomic_reference_protein_in_cluster'] = np.where(df['cluster_members'].isin(cluster_reps_with_genomic_references_list), 'yes', 'no')

    # Determine contigsDB type: metagenome or genomes
    # FIXME: This will need to be changed in the future to accomidate SAGs, MAGs, and other genomic sources
    # If metagenome_name is in external_genomes_names_list then it's a genome
    contigsDB_type_dict = {}
    for name in list(df.cluster_members):
        if any(x in name for x in M.external_genomes_names_list):
            contigsDB_type_dict[name] = "genome"
        else:
            contigsDB_type_dict[name] = "metagenome"
        
    df["contig_db_type"] = df.cluster_members.apply(lambda name: contigsDB_type_dict[name])

    # grab the final columns
    df = df[['split_name', 'contig_db_type', 'has_genomic_reference_protein_in_cluster', 'cluster_size']]

    # Export
    #-------
    df.to_csv(output, \
              sep="\t", \
              index=None, \
              na_rep="NA")

def count_cluster_size(group):
    c = group['cluster_members'].count()
    group['cluster_size'] = c

    return group


rule make_misc_data:
    """Make misc data file for sequences"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "add_contigsDB_type_{HMM}.log")
    input:
        final_list_of_sequences_for_mapping_headers = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_headers.tmp")
    output:
        misc_data_final = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_misc.tsv")
    params:
        mmseqs_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-mmseqs_NR_cluster.tsv"),
        coverage_cluster_rep_index = os.path.join(dirs_dict['RIBOSOMAL_PROTEIN_FASTAS'], "{HMM}", "{HMM}-coverage_cluster.tsv")
    threads: M.T('add_misc_data_to_taxonomy')
    run:
        """Here we determine the origin of each SCG (which kind of contigsDB): metagenome, isolate genome, etc."""
        if M.cluster_representative_method == "mmseqs":
            extract_misc_data(mmseqs_cluster_rep_index = params.mmseqs_cluster_rep_index,
                              final_sequences_headers = input.final_list_of_sequences_for_mapping_headers,
                              output = output.misc_data_final)
        if M.cluster_representative_method == "cluster_rep_with_coverages":
            extract_misc_data(mmseqs_cluster_rep_index = params.coverage_cluster_rep_index,
                              final_sequences_headers = input.final_list_of_sequences_for_mapping_headers,
                              output = output.misc_data_final)



rule anvi_scg_taxonomy:
    """Run anvi-estimate-SCG-taxonomy and import the resulting taxonomy misc data to profileDB for internal HMMs only"""

    version: 1.0
    log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_scg_taxonomy_{HMM}.log")
    input:
        misc_data = rules.make_misc_data.output.misc_data_final,
        reformat_file = rules.cat_sequences_to_one_fasta.output.reformat_report_all,
    params:
        combined_genomes_txt = "combined_genomes.txt",
        taxonomy = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_estimate_scg_taxonomy_results"),
        taxonomy_long = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_estimate_scg_taxonomy_results-RAW-LONG-FORMAT.txt"),
        final_list_of_sequences_for_mapping_headers = os.path.join(dirs_dict['MSA'], "{HMM}", "{HMM}_headers.tmp"),
        tax_data_final = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_scg_taxonomy_data.tsv"),
    output: 
        # tax_data_final = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_scg_taxonomy_data.tsv"),
        done = touch(os.path.join("ECOPHYLO_WORKFLOW", "{HMM}_anvi_estimate_scg_taxonomy_for_SCGs.done"))
    threads: M.T('anvi_scg_taxonomy')
    run:
        HMM_source = M.HMM_source_dict[wildcards.HMM]
        
        if HMM_source in M.internal_HMM_sources:

            shell("anvi-estimate-scg-taxonomy -M {params.combined_genomes_txt} \
                                              --metagenome-mode \
                                              --scg-name-for-metagenome-mode {wildcards.HMM} \
                                              -T {threads} \
                                              --raw-output \
                                              -O {params.taxonomy} &> {log}")
            # Import data
            #------------
            scg_taxonomy = pd.read_csv(params.taxonomy_long, \
                                       sep="\t", \
                                       index_col=False)
                                            
            reformat_report = pd.read_csv(input.reformat_file, \
                                          sep="\t", \
                                          index_col=False, \
                                          names=["new_header", "header"])

            final_sequences_headers = pd.read_csv(params.final_list_of_sequences_for_mapping_headers, \
                                                sep="\t", \
                                                index_col=False, \
                                                names=["identifier"])
            
            # Clean Data
            #-----------
            reformat_report = pd.merge(reformat_report, final_sequences_headers, left_on='new_header', right_on='identifier', how='inner')
            reformat_report["gene_callers_id"] = reformat_report['header'].str.split("gene_callers_id:|\|start:", expand=True)[1].astype(str)
            reformat_report["new_header_tmp"] = reformat_report['new_header'].str.rsplit('_', 1).str[0]
            reformat_report["identifier"] = reformat_report["new_header_tmp"] + "_" + reformat_report["gene_callers_id"] 
            scg_taxonomy["identifier"] =  scg_taxonomy["metagenome_name"] + "_" +  scg_taxonomy["gene_name"] + "_" +  scg_taxonomy["gene_callers_id"].astype(str)
            scg_taxonomy = scg_taxonomy.merge(reformat_report, on='identifier', how="inner")
            scg_taxonomy['split_name'] = scg_taxonomy['new_header'].astype(str) + '_split_00001' 
            scg_taxonomy = scg_taxonomy[["split_name", "identifier", "percent_identity", "t_domain", "t_phylum", "t_class", "t_order", "t_family", "t_genus", "t_species"]]

            # Export
            #-------
            scg_taxonomy.to_csv(params.tax_data_final, \
                                sep="\t", \
                                index=None, \
                                na_rep="NA")

        else:
            pass




if M.samples_txt_file:
    rule make_metagenomics_config_file:
        """Make a METAGENOMICS WORKFLOW config.json customized for ECOPHYLO_WORKFLOW"""

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "make_metagenomics_config_file.log")
        input:
            rules.make_fasta_txt.output.fasta_txt
        output:
            config = os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW", "metagenomics_config.json")
        threads: M.T('make_metagenomics_config_file')
        run:

            shell('anvi-run-workflow -w metagenomics --get-default-config {output.config}')

            config = open(output.config)
            config_dict = json.load(config)
            config_dict['fasta_txt'] = 'fasta.txt'
            sample_txt_path = M.samples_txt_file
            config_dict['samples_txt'] = sample_txt_path
            config_dict['references_mode'] = True
            config_dict['anvi_run_hmms']['run'] = False
            config_dict["anvi_script_reformat_fasta"]['run'] = False
            config_dict['anvi_run_kegg_kofams']['run'] = False
            config_dict['anvi_run_ncbi_cogs']['run'] = False
            config_dict['anvi_run_scg_taxonomy']['run'] = False
            config_dict['anvi_run_trna_scan']['run'] = False
            config_dict['anvi_run_scg_taxonomy']['run'] = False
            config_dict['iu_filter_quality_minoche']['run'] = False
            config_dict['anvi_profile']['--min-contig-length'] = 0
            config_dict['bowtie']['threads'] = 5
            config_dict['bowtie_build']['threads'] = 5
            config_dict['anvi_gen_contigs_database']['threads'] = 5

            if M.clusterize_metagenomics_workflow == True:
                config_dict['bowtie']['threads'] = 10
                config_dict['anvi_profile']['threads'] = 10
                config_dict['anvi_merge']['threads'] = 10
            else:
                pass

            with open(output.config, "w") as outfile:
                json.dump(config_dict, outfile, indent=4)


    rule run_metagenomics_workflow:
        """Run metagenomics workflow to profile HMM_hits"""

        version: 1.0
        log: "ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/00_LOGS/run_metagenomics_workflow.log"
        input:
            config = rules.make_metagenomics_config_file.output.config,
        output:
            done = touch(os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW", "metagenomics_workflow.done"))
        params:
            HPC_string = M.metagenomics_workflow_HPC_string
        threads: M.T('run_metagenomics_workflow')
        run:

            # Convert r1 and r2 to absolute paths
            samples_txt_new_path = os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/", M.samples_txt_file)
            samples_txt_new = pd.read_csv(M.samples_txt_file, sep='\t', index_col=False)
            samples_txt_new['r1'] = samples_txt_new['r1'].apply(lambda x: os.path.abspath(str(x)))
            samples_txt_new['r2'] = samples_txt_new['r2'].apply(lambda x: os.path.abspath(str(x)))
            samples_txt_new.to_csv(samples_txt_new_path, sep="\t", index=False, header=True)

            shell("mkdir -p METAGENOMICS_WORKFLOW/00_LOGS && touch {log}")

            if M.clusterize_metagenomics_workflow == True:
                shell('cd ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/ && anvi-run-workflow -w metagenomics -c metagenomics_config.json --additional-params --cluster \'clusterize -j={{rule}} -o={{log}} -n={{threads}} -x\' --cores 200 --resource nodes=200 --latency-wait 100 --keep-going --rerun-incomplete &> {log} && cd -')
            elif M.metagenomics_workflow_HPC_string:
                HPC_string = params.HPC_string
                shell(f'cd ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/ && anvi-run-workflow -w metagenomics -c metagenomics_config.json --additional-params --cluster "{HPC_string}" --cores 201 --resource nodes=200 --latency-wait 100 --keep-going --rerun-incomplete &> {log} && cd -')
            else:
                shell("cd ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/ && anvi-run-workflow -w metagenomics -c metagenomics_config.json -A --rerun-incomplete --latency-wait 100 --keep-going && cd -")
            

    rule add_default_collection:
        """"""

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "add_default_collection_{HMM}.log")
        input: metagenomics_workflow_done = os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW", "metagenomics_workflow.done")
        params:
            contigsDB = ancient(os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/03_CONTIGS", "{HMM}.db")),
            profileDB = os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/06_MERGED", "{HMM}", "PROFILE.db")
        output: touch(os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW", "{HMM}_add_default_collection.done"))
        threads: M.T('add_default_collection')
        shell:
            """
            anvi-script-add-default-collection -c {params.contigsDB} \
                                            -p {params.profileDB}
            """

    rule anvi_summarize:
        """
        Get coverage values for HMM_hits
        """

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_summarize_{HMM}.log")
        input: 
            done = os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW", "{HMM}_add_default_collection.done")
        params:
            contigsDB = ancient(os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/03_CONTIGS", "{HMM}-contigs.db")),
            profileDB = os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/06_MERGED", "{HMM}", "PROFILE.db"),
            output_dir = os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/07_SUMMARY", "{HMM}")
        output: touch(os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/07_SUMMARY", "{HMM}_summarize.done"))
        threads: M.T('anvi_summarize')
        shell:
            """
            anvi-summarize -c {params.contigsDB} \
                        -p {params.profileDB} \
                        -o {params.output_dir} \
                        -C DEFAULT \
                        --init-gene-coverages \
                        --just-do-it;
            """
            
    rule make_anvio_state_file:
        """Make a state file customized for EcoPhylo workflow interactive interface"""

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "make_anvio_state_file_{HMM}.log")
        input:
            num_tree_tips = rules.subset_DNA_reps_with_QCd_AA_reps_for_mapping.output.NT_for_mapping,
            done_scg = rules.anvi_scg_taxonomy.output.done
        params:
            tax_data_final = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_scg_taxonomy_data.tsv"),
            misc_data_final = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_misc.tsv"),
        output:
            state_file = os.path.join("ECOPHYLO_WORKFLOW", "{HMM}_ECOPHYLO_WORKFLOW_state.json")
        threads: M.T('make_anvio_state_file')
        run:

            HMM_source = M.HMM_source_dict[wildcards.HMM]

            # Read in misc data headers for layer_order
            if HMM_source in M.internal_HMM_sources:
                with open(params.tax_data_final) as f:
                    lines = f.read()
                    first = lines.split('\n', 1)[0]
                scg_taxonomy_layers_list = first.split("\t")

            with open(params.misc_data_final) as f:
                lines = f.read()
                first = lines.split('\n', 1)[0]
            misc_layers_list = first.split("\t")

            state_dict = {}

            # basics
            state_dict['version'] = '3'
            state_dict['tree-type'] = 'phylogram'
            state_dict['current-view'] = 'mean_coverage'

            # height and width
            # FIXME: It's unclear to me how the interactive interface determines
            # height and width of a tree when the input value is 0. There has to 
            # be some kind of calculation to determine the tree shape in the backend
            # of the interface because even after I export a "default" state file
            # the height and width are still "0". However, if you change the height and width
            # values within the interface to "" the tree will disappear. I need to sort this 
            # out eventually to have a clean way of changing the tree shape to 
            # match the dimensions of the number of SCGs vs metagenomes. 
            # num_tree_tips = pd.read_csv(input.num_tree_tips, \
            #                             sep="\t", \
            #                             index_col=None)


            # layer-orders
            first_layers = ["__parent__", "length", "gc_content"]
            metagenomes = []

            for metagenome in M.sample_names_for_mapping_list:
                metagenomes.append(metagenome)

            if HMM_source in M.internal_HMM_sources:
                layer_order = first_layers + metagenomes + misc_layers_list + scg_taxonomy_layers_list 
            else:
                layer_order = first_layers + metagenomes + misc_layers_list 

            state_dict['layer-order'] = layer_order

            # layers
            layers_dict = {}

            metagenome_layers_dict = {}

            metagenome_attributes = {
                "color": "#000000",
                "height": "180",
                "margin": "15",
                "type": "bar",
                "color-start": "#FFFFFF"
                }

            for metagenome in metagenomes:
                metagenome_layers_dict[str(metagenome)] = metagenome_attributes

            layer_attributes_parent = {
                "color": "#000000",
                "height": "0",
                "margin": "15",
                "type": "color",
                "color-start": "#FFFFFF"
                }

            length = {
                "color": "#000000",
                "height": "0",
                "margin": "15",
                "type": "color",
                "color-start": "#FFFFFF"
                }

            gc_content = {
                "color": "#000000",
                "height": "0",
                "margin": "15",
                "type": "color",
                "color-start": "#FFFFFF"
                }

            identifier = {
                "color": "#000000",
                "height": "0",
                "margin": "15",
                "type": "color",
                "color-start": "#FFFFFF"
                }

            percent_identity = {
                "color": "#000000",
                "height": "180",
                "margin": "15",
                "type": "line",
                "color-start": "#FFFFFF"
                }
                
            layers_dict.update(metagenome_layers_dict)
            layers_dict['__parent__'] = layer_attributes_parent
            layers_dict['length'] = length
            layers_dict['gc_content'] = gc_content
            layers_dict['identifier'] = identifier
            layers_dict['percent_identity'] = percent_identity

            state_dict['layers'] = layers_dict

            # views
            views_dict = {}

            mean_coverage_dict = {}

            false = False
            percent_identity = {
                "normalization": "none",
                "min": {
                    "value": "90",
                    "disabled": false 
                    },
                "max": {
                    "value": "100",
                    "disabled": false
                    }
            }

            mean_coverage_dict['percent_identity'] = percent_identity 
            views_dict['mean_coverage'] = mean_coverage_dict
            state_dict['views'] = views_dict

            with open(output.state_file, "w") as outfile:
                    json.dump(state_dict, outfile, indent=4)

    rule anvi_import_everything_metagenome:
        """
        Import state file, phylogenetic tree, AND misc data to interactive interface

        If samples.txt is NOT provided then we will make an Ad Hoc profileDB for the tree to import misc data
        """

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_import_state_{HMM}.log")
        input:
            tree = rules.rename_tree_tips.output.tree,
            misc_data = rules.make_misc_data.output.misc_data_final,
            state = rules.make_anvio_state_file.output,
            done = rules.run_metagenomics_workflow.output.done
        params:
            tax_data_final = rules.anvi_scg_taxonomy.params.tax_data_final,
            profileDB = os.path.join("ECOPHYLO_WORKFLOW/METAGENOMICS_WORKFLOW/06_MERGED", "{HMM}", "PROFILE.db"),
            tree_profileDB = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}-PROFILE.db")
        output: 
            touch(os.path.join("ECOPHYLO_WORKFLOW", "{HMM}_state_imported.done")),

        threads: M.T('anvi_import_state')
        run:
            state = os.path.join("ECOPHYLO_WORKFLOW", "{wildcards.HMM}_ECOPHYLO_WORKFLOW_state.json")

            shell(f"anvi-import-state -p {params.profileDB} -s {state} -n default")

            shell("anvi-import-items-order -p {params.profileDB} -i {input.tree} --name {wildcards.HMM}_tree")

            shell("anvi-import-misc-data -p {params.profileDB} --target-data-table items {input.misc_data} --just-do-it")

            HMM_source = M.HMM_source_dict[wildcards.HMM]
            
            if HMM_source in M.internal_HMM_sources:
                shell("anvi-import-misc-data -p {params.profileDB} --target-data-table items {params.tax_data_final} --just-do-it")

else:

    rule make_anvio_state_file_tree:
        """Make a state file customized for EcoPhylo workflow interactive interface"""

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "make_anvio_state_file_{HMM}.log")
        input:
            num_tree_tips = rules.subset_DNA_reps_with_QCd_AA_reps_for_mapping.output.NT_for_mapping,
            done_scg = rules.anvi_scg_taxonomy.output.done
        params:
            tax_data_final = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_scg_taxonomy_data.tsv"),
            misc_data_final = os.path.join(dirs_dict['MISC_DATA'], "{HMM}_misc.tsv"),
        output:
            state_file = os.path.join("ECOPHYLO_WORKFLOW", "{HMM}_TREE_state.json")
        threads: M.T('make_anvio_state_file')
        run:

            HMM_source = M.HMM_source_dict[wildcards.HMM]

            # Read in misc data headers for layer_order
            if HMM_source in M.internal_HMM_sources:
                with open(params.tax_data_final) as f:
                    lines = f.read()
                    first = lines.split('\n', 1)[0]
                scg_taxonomy_layers_list = first.split("\t")

            with open(params.misc_data_final) as f:
                lines = f.read()
                first = lines.split('\n', 1)[0]
            misc_layers_list = first.split("\t")

            state_dict = {}

            # basics
            state_dict['version'] = '3'
            state_dict['tree-type'] = 'phylogram'
            state_dict['current-view'] = 'single'

            # height and width
            # FIXME: It's unclear to me how the interactive interface determines
            # height and width of a tree when the input value is 0. There has to 
            # be some kind of calculation to determine the tree shape in the backend
            # of the interface because even after I export a "default" state file
            # the height and width are still "0". However, if you change the height and width
            # values within the interface to "" the tree will disappear. I need to sort this 
            # out eventually to have a clean way of changing the tree shape to 
            # match the dimensions of the number of SCGs vs metagenomes. 
            # num_tree_tips = pd.read_csv(input.num_tree_tips, \
            #                             sep="\t", \
            #                             index_col=None)


            # layer-orders
            if HMM_source in M.internal_HMM_sources:
                layer_order = misc_layers_list + scg_taxonomy_layers_list 
            else:
                layer_order = misc_layers_list 

            state_dict['layer-order'] = layer_order

            # layers
            layers_dict = {}

            length = {
                "color": "#000000",
                "height": "0",
                "margin": "15",
                "type": "color",
                "color-start": "#FFFFFF"
                }

            names = {
                "color": "#000000",
                "height": "0",
                "margin": "15",
                "type": "color",
                "color-start": "#FFFFFF"
                }

            percent_identity = {
                "color": "#000000",
                "height": "180",
                "margin": "15",
                "type": "line",
                "color-start": "#FFFFFF"
                }
                
            layers_dict['percent_identity'] = percent_identity
            state_dict['layers'] = layers_dict

            # views
            views_dict = {}

            single_dict = {}

            percent_identity = {
                "normalization": "none",
                "min": {
                    "value": "90",
                    "disabled": "false"
                    },
                "max": {
                    "value": "100",
                    "disabled": "false"
                    }
            }

            single_dict['percent_identity'] = percent_identity 
            views_dict['single'] = single_dict
            state_dict['views'] = views_dict

            with open(output.state_file, "w") as outfile:
                    json.dump(state_dict, outfile, indent=4)

    rule anvi_import_everything_tree:
        """
        Import state file, phylogenetic tree, AND misc data to interactive interface

        If samples.txt is NOT provided then we will make an Ad Hoc profileDB for the tree to import misc data
        """

        version: 1.0
        log: os.path.join(dirs_dict['LOGS_DIR'], "anvi_import_state_{HMM}.log")
        input:
            tree = rules.rename_tree_tips.output.tree,
            misc_data = rules.make_misc_data.output.misc_data_final,
            state = rules.make_anvio_state_file_tree.output.state_file
        params:
            tax_data_final = rules.anvi_scg_taxonomy.params.tax_data_final,
            tree_profileDB = os.path.join(dirs_dict['TREES'], "{HMM}", "{HMM}-PROFILE.db")
        output: 
            touch(os.path.join("ECOPHYLO_WORKFLOW", "{HMM}_state_imported.done")),

        threads: M.T('anvi_import_state')
        run:
            # Make place holder profileDB for tree
            import anvio
            import anvio.utils as utils
            import anvio.filesnpaths as filesnpaths
            from anvio.dbops import ProfileSuperclass, ProfileDatabase

            filesnpaths.is_file_exists(input.tree)
            newick_tree_text = ''.join([l.strip() for l in open(os.path.abspath(input.tree)).readlines()])

            p_meta = {}
            views = {}

            p_meta['output_dir'] = None
            p_meta['views'] = {}
            p_meta['db_type'] = 'profile'
            p_meta['merged'] = True
            p_meta['blank'] = True
            p_meta['default_view'] = 'single'

            p_meta['item_orders'] = {}
            p_meta['available_item_orders'] = []
            p_meta['default_item_order'] = []

            item_order_name = '%s:unknown:unknown' % filesnpaths.get_name_from_file_path(input.tree)
            p_meta['default_item_order'] = item_order_name
            p_meta['available_item_orders'].append(item_order_name)
            p_meta['item_orders'][item_order_name] = {'type': 'newick', 'data': newick_tree_text}

            p_meta['default_view'] = "single"
            p_meta['samples'] = "names"
            p_meta['sample_id'] = 'AD HOC DISPLAY'

            profile_db = ProfileDatabase(params.tree_profileDB)
            profile_db.create({'db_type': 'profile',
                                'db_variant': 'ad-hoc-display',
                                'blank': True,
                                'merged': True,
                                'contigs_db_hash': None,
                                'items_ordered': False,
                                'samples': ', '.join(p_meta['samples']),
                                'sample_id': p_meta['sample_id']})

            shell("anvi-import-items-order -p {params.tree_profileDB} -i {input.tree} --name {wildcards.HMM}_tree")

            shell("anvi-import-misc-data -p {params.tree_profileDB} --target-data-table items {input.misc_data} --just-do-it")

            HMM_source = M.HMM_source_dict[wildcards.HMM]
            
            if HMM_source in M.internal_HMM_sources:
                shell("anvi-import-misc-data -p {params.tree_profileDB} --target-data-table items {params.tax_data_final} --just-do-it")
            
                shell("anvi-import-state -p {params.tree_profileDB} -s {input.state} -n default")